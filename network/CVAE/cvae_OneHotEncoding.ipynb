{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zapniU98wzMW"
      },
      "source": [
        "# Conditional variational autoencoder\n",
        "Variational autoencoder for tabular data, oriented upon: https://lschmiddey.github.io/fastpages_/2021/03/14/tabular-data-variational-autoencoder.html \n",
        "\n",
        "Adopted with one hot encoding for tabular data\n",
        "## Load Json Database of recipes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2zTkqc12wzMc",
        "outputId": "7890cde5-ae13-49f9-f9e1-fbb6d5758573"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "dataPath = '/content/drive/MyDrive/TP2/Datasets/Recipe1M/'\n",
        "import sys\n",
        "sys.path.append(dataPath)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_e61O0YAwzMd"
      },
      "outputs": [],
      "source": [
        "dataPath = 'data/'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FkkejbegwzMd",
        "outputId": "cb43cd35-8254-4d08-9609-b317cb34581c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['title', 'ingredients', 'instructions'], dtype='object')\n",
            "100000\n"
          ]
        }
      ],
      "source": [
        "import pickle\n",
        "import pandas as pd\n",
        "\n",
        "with open(dataPath + '2022_02_11/recipes_valid_0.pkl', 'rb') as f:\n",
        "    pklData = pd.DataFrame(pickle.load(f))\n",
        "\n",
        "print(pklData.keys())\n",
        "print(len(pklData))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3woTtbf9wzMe"
      },
      "source": [
        "## Convert list of ingredients to pandas dataframe and one hot encode the dataframe."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install word2vec"
      ],
      "metadata": {
        "id": "A7kjo-mMGYBU",
        "outputId": "54160442-e7cb-427b-f880-d89ce5db641d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: word2vec in /usr/local/lib/python3.7/dist-packages (0.11.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from word2vec) (1.1.0)\n",
            "Requirement already satisfied: numpy>=1.9.2 in /usr/local/lib/python3.7/dist-packages (from word2vec) (1.21.5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "d1HzhTL5EUPd"
      },
      "outputs": [],
      "source": [
        "from ReciMePreprocessor import ReciMePreprocessor\n",
        "\n",
        "preprocessor = ReciMePreprocessor(dataPath + '/vocab.bin')\n",
        "\n",
        "out = preprocessor.preProcessInput(pklData['ingredients'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CBKbgRf-wzMg"
      },
      "source": [
        "## VAE\n",
        "### Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a5CjNvLnwzMg",
        "outputId": "a636b2c0-a3cf-4d1a-ffe4-3ffe6f43d5ad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "# Import pytorch dependencies\n",
        "import torch\n",
        "from torch import optim\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Import additional libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Import custom autoencoder\n",
        "from ReciMeEncoder import ReciMeEncoder, RmeParameters\n",
        "\n",
        "# Import custom helper functions\n",
        "from networkUtils import DataBuilder, CustomLoss, standardize_data, sparse_batch_collate\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DR6u1q3lwzMg"
      },
      "source": [
        "### Setup Datasets + Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "yle-AZJ6wzMh"
      },
      "outputs": [],
      "source": [
        "# One hot encoding without embedding and using sparse frame\n",
        "train_data, test_data, scaler =  standardize_data(out)\n",
        "batch_size = 1024\n",
        "# Train/Testdataset split is defined in the DataBuilder\n",
        "traindata_set=DataBuilder(train_data, standardizer=scaler)\n",
        "testdata_set=DataBuilder(test_data, standardizer=scaler)\n",
        "# Definition of batches\n",
        "trainloader=DataLoader(dataset=traindata_set,batch_size=batch_size)\n",
        "testloader=DataLoader(dataset=testdata_set,batch_size=batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "testdata_set.x.shape[1]"
      ],
      "metadata": {
        "id": "1n_eBmw9Pgp4",
        "outputId": "39f228f4-c111-4f7d-a316-b230213441fa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6360"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "oD_T2VYFwzMh"
      },
      "outputs": [],
      "source": [
        "params = RmeParameters(testdata_set.x.shape[1], 8192, 4096, 2048, 512)\n",
        "\n",
        "model = ReciMeEncoder(params).to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
        "loss_mse = CustomLoss()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CH1FyiqXwzMi"
      },
      "source": [
        "### Train Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "zkHZycy2wzMi"
      },
      "outputs": [],
      "source": [
        "epochs = 50\n",
        "log_interval = 5\n",
        "val_losses = []\n",
        "train_losses = []\n",
        "test_losses = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "dyZzQCtwwzMj"
      },
      "outputs": [],
      "source": [
        "def train(epoch):\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "    train_loss_MSE = 0\n",
        "    train_loss_KLD = 0\n",
        "    for batch_idx, data in enumerate(trainloader):\n",
        "        data = data.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        recon_batch, mu, logvar = model(data)\n",
        "        loss_MSE, loss_KLD, loss = loss_mse(recon_batch, data, mu, logvar)\n",
        "        loss.backward()\n",
        "        loss_itm = loss.item()\n",
        "        train_loss += loss_itm\n",
        "        train_loss_MSE += loss_MSE\n",
        "        train_loss_KLD += loss_KLD\n",
        "        optimizer.step()\n",
        "    if epoch % log_interval == 0:        \n",
        "        print('====> Epoch: {} Average training loss: {:.5f}, MSE: {:.5f}, KLD: {:.5f}'.format(\n",
        "            epoch, train_loss / len(trainloader.dataset), \n",
        "            train_loss_MSE / len(trainloader.dataset), \n",
        "            train_loss_KLD / len(trainloader.dataset)))\n",
        "        train_losses.append(train_loss / len(trainloader.dataset))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "NtNNh601wzMj"
      },
      "outputs": [],
      "source": [
        "def test(epoch):\n",
        "    with torch.no_grad():\n",
        "        test_loss = 0\n",
        "        test_loss_MSE = 0\n",
        "        test_loss_KLD = 0\n",
        "        for batch_idx, data in enumerate(testloader):\n",
        "            data = data.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            recon_batch, mu, logvar = model(data)\n",
        "            loss_MSE, loss_KLD, loss = loss_mse(recon_batch, data, mu, logvar)\n",
        "            loss_itm = loss.item()\n",
        "            test_loss += loss_itm\n",
        "            test_loss_MSE += loss_MSE\n",
        "            test_loss_KLD += loss_KLD\n",
        "        if epoch % log_interval == 0:        \n",
        "            print('====> Epoch: {} Average training loss: {:.4f}, MSE: {:.4f}, KLD: {:.4f}'.format(\n",
        "                epoch, test_loss / len(testloader.dataset), \n",
        "                test_loss_MSE / len(testloader.dataset), \n",
        "                test_loss_KLD / len(testloader.dataset)))\n",
        "            test_losses.append(test_loss / len(testloader.dataset))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ascBDVv1wzMk",
        "outputId": "e863e643-76af-4040-80c1-28afc6697987"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "====> Epoch: 5 Average training loss: 0.03682, MSE: 0.00841, KLD: 0.02841\n",
            "====> Epoch: 5 Average training loss: 0.0355, MSE: 0.0085, KLD: 0.0269\n",
            "====> Epoch: 10 Average training loss: 0.02297, MSE: 0.00841, KLD: 0.01456\n",
            "====> Epoch: 10 Average training loss: 0.0232, MSE: 0.0085, KLD: 0.0146\n",
            "====> Epoch: 15 Average training loss: 0.01735, MSE: 0.00841, KLD: 0.00893\n",
            "====> Epoch: 15 Average training loss: 0.0177, MSE: 0.0085, KLD: 0.0092\n",
            "====> Epoch: 20 Average training loss: 0.01417, MSE: 0.00841, KLD: 0.00575\n",
            "====> Epoch: 20 Average training loss: 0.0147, MSE: 0.0085, KLD: 0.0061\n",
            "====> Epoch: 25 Average training loss: 0.01224, MSE: 0.00841, KLD: 0.00382\n",
            "====> Epoch: 25 Average training loss: 0.0127, MSE: 0.0085, KLD: 0.0042\n",
            "====> Epoch: 30 Average training loss: 0.01102, MSE: 0.00841, KLD: 0.00261\n",
            "====> Epoch: 30 Average training loss: 0.0115, MSE: 0.0085, KLD: 0.0029\n",
            "====> Epoch: 35 Average training loss: 0.01023, MSE: 0.00841, KLD: 0.00181\n",
            "====> Epoch: 35 Average training loss: 0.0106, MSE: 0.0085, KLD: 0.0021\n"
          ]
        }
      ],
      "source": [
        "for epoch in range(1,epochs+1):\n",
        "    train(epoch)\n",
        "    test(epoch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "stl8-ACfwzMk"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.plot(test_losses)\n",
        "plt.plot(train_losses)\n",
        "plt.legend(['Test', 'Train'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YYvu9HekwzMm"
      },
      "source": [
        "### Draw random samples form latent space and generate new data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xxoWJ_IWEUPj"
      },
      "outputs": [],
      "source": [
        "with torch.no_grad():\n",
        "    for batch_idx, data in enumerate(testloader):\n",
        "        data = data.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        recon_batch, mu, logvar = model(data)\n",
        "        break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o-l_00KswzMn"
      },
      "outputs": [],
      "source": [
        "sigma = torch.exp(logvar/2)\n",
        "# sample z from q\n",
        "no_samples = 20\n",
        "q = torch.distributions.Normal(mu.mean(axis=0), sigma.mean(axis=0))\n",
        "z = q.rsample(sample_shape=torch.Size([no_samples]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hXeISFOtwzMn"
      },
      "outputs": [],
      "source": [
        "scaler = trainloader.dataset.standardizer\n",
        "\n",
        "with torch.no_grad():\n",
        "    pred = model.decode(z).cpu().numpy()\n",
        "\n",
        "fake_data = scaler.inverse_transform(pred)\n",
        "df_fake = pd.DataFrame(fake_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QQxp_k9FEUPk"
      },
      "outputs": [],
      "source": [
        "print(fake_data.shape)\n",
        "print(type(fake_data))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fGIuz8vJEUPk"
      },
      "outputs": [],
      "source": [
        "def inverseOneHotEncoding(encoded: np.ndarray, dictionary: dict) -> np.ndarray:\n",
        "    output = []\n",
        "    for row in encoded:\n",
        "        rowTransformed = np.reshape(row, (20,-1))\n",
        "        rowString = []\n",
        "        indexTransformed = np.argmax(rowTransformed, axis=1) + 1\n",
        "        for index in indexTransformed:\n",
        "            if (index in dictionary.values()):\n",
        "                rowString.append(list(dictionary.keys())[list(dictionary.values()).index(index)])\n",
        "            else:\n",
        "                rowString.append(\"\")\n",
        "        output.append(rowString)\n",
        "    return np.array(output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cX5P1wV6EUPl"
      },
      "outputs": [],
      "source": [
        "def inverseEmbedding(embedded: np.ndarray, dictionary: dict) -> np.ndarray:\n",
        "    output = []\n",
        "    for row in embedded:\n",
        "        outputRows = []\n",
        "        for rowTransformed in np.reshape(row, (20,-1)):\n",
        "            rowTransformed = torch.Tensor(rowTransformed)\n",
        "            distance = torch.norm(preprocessor.emb.weight.data - rowTransformed, dim=1)\n",
        "            nearest = torch.argmin(distance)\n",
        "            index = nearest.item()\n",
        "            if index:\n",
        "                outputRows.append(list(dictionary.keys())[list(dictionary.values()).index(index)])\n",
        "            else:\n",
        "                outputRows.append(\"\")\n",
        "        output.append(outputRows)\n",
        "\n",
        "    return np.array(output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nz90yVggEUPm"
      },
      "outputs": [],
      "source": [
        "def decodeOutput(output: np.ndarray):\n",
        "    # Split output into amounts, units and ingredients \n",
        "    amounts = output[:, :20]\n",
        "    amountColumns = ['amount_' + str(sub) for sub in list(range(0,20))]\n",
        "    units = output[:, 20:len(preprocessor.unitDict)*20+20]\n",
        "    unitColumns = ['unit_' + str(sub) for sub in list(range(0,20))]\n",
        "    ingredients = output[:, len(preprocessor.unitDict)*20+20:]\n",
        "    ingredientColumns = ['ingredient_' + str(sub) for sub in list(range(0,20))]\n",
        "    unitsDecoded = inverseOneHotEncoding(units, preprocessor.unitDict)\n",
        "    ingredientsDecoded = inverseEmbedding(ingredients, preprocessor.ingredientDict)\n",
        "    outputFrame = []\n",
        "    for index in range(len(amounts)):\n",
        "        array = np.stack((amounts[index], unitsDecoded[index], ingredientsDecoded[index]),axis=1)\n",
        "        outputFrame.append(pd.DataFrame(array, columns=[\"amount\", \"unit\", \"ingredient\"]))\n",
        "    return outputFrame\n",
        "\n",
        "df = decodeOutput(fake_data)\n",
        "df[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2dh-6TeiEUPm"
      },
      "outputs": [],
      "source": [
        "data = pklData[:1]['ingredients']\n",
        "\n",
        "dataEmbedded = np.array(preprocessor.preProcessInput(data))\n",
        "\n",
        "dataReconverted = decodeOutput(dataEmbedded)\n",
        "pd.concat([dataReconverted[0], data[0].add_prefix(\"orig_\")], axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V6pqjuU7EUPm"
      },
      "outputs": [],
      "source": [
        "string = \"dry dill weed\"\n",
        "name_words = string.lower().split(' ')\n",
        "for i in range(len(name_words)):\n",
        "    print('_'.join(name_words[i:]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XlXnAae3EUPn"
      },
      "outputs": [],
      "source": [
        "embeddings = torch.nn.Embedding(1000, 100)\n",
        "my_sample = torch.randn(1, 100)\n",
        "distance = torch.norm(embeddings.weight.data - my_sample, dim=1)\n",
        "nearest = torch.argmin(distance)\n",
        "print(my_sample.shape)\n",
        "print(embeddings.weight.data.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NQ6kN0hQwzMo"
      },
      "outputs": [],
      "source": [
        "df_fake_stripped = df_fake.drop(columns=dropColumns)\n",
        "df_fake_stripped_decoded = pd.DataFrame(data=enc.inverse_transform(df_fake_stripped), columns=frameStripped_cols)\n",
        "df_fake_decoded = pd.concat([df_fake[dropColumns], df_fake_stripped_decoded], axis=1)\n",
        "df_fake_decoded"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jbh1MGDCwzMo"
      },
      "outputs": [],
      "source": [
        "class Ingredient:\n",
        "    def __init__(self, amount, unit, ingredient) -> None:\n",
        "        self.amount = amount\n",
        "        self.unit = unit\n",
        "        self.ingredient = ingredient\n",
        "\n",
        "    def __repr__(self) -> str:\n",
        "        return \"\\nAmount: \" + str(self.amount) + \"\\n Unit: \" + str(self.unit) + \"\\n Ingredient: \" + str(self.ingredient)\n",
        "\n",
        "recipes = []\n",
        "lenIngredients = int(len(df_fake_decoded.columns)/3)\n",
        "for value in df_fake_decoded.values:\n",
        "    ingredients = []\n",
        "    for index in range(0,lenIngredients):\n",
        "        frame = []\n",
        "        frame.append(value[index])\n",
        "        frame.append(value[(2*index)+lenIngredients])\n",
        "        frame.append(value[(2*index+1)+lenIngredients])\n",
        "        ingredients.append(frame)\n",
        "    recipes.append(ingredients)\n",
        "\n",
        "pd.DataFrame(recipes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lNbXCbGPwzMp"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "name": "cvae.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "bb875f4a3a3b27879a30076b84fa05114b199e04f8f3aeee4f2ba73a2af38a98"
    },
    "kernelspec": {
      "display_name": "Python 3.6.9 64-bit ('recime': conda)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.11"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}