{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pytorch_lightning --quiet\n",
    "# !pip install ray[tune] --quiet\n",
    "!pip install transformers --quiet\n",
    "!pip install \"ray[tune]\" torch torchvision pytorch-lightning --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google.colab'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32md:\\02_Studium\\SBX\\mad-recime\\network\\LSTM\\src\\nlp_utils\\trainTitle.ipynb Cell 1'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/02_Studium/SBX/mad-recime/network/LSTM/src/nlp_utils/trainTitle.ipynb#ch0000011?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mgoogle\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcolab\u001b[39;00m \u001b[39mimport\u001b[39;00m drive\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/02_Studium/SBX/mad-recime/network/LSTM/src/nlp_utils/trainTitle.ipynb#ch0000011?line=1'>2</a>\u001b[0m drive\u001b[39m.\u001b[39mmount(\u001b[39m'\u001b[39m\u001b[39m/content/drive/\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/02_Studium/SBX/mad-recime/network/LSTM/src/nlp_utils/trainTitle.ipynb#ch0000011?line=3'>4</a>\u001b[0m rootDir \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m/content/drive/MyDrive/\u001b[39m\u001b[39m'\u001b[39m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'google.colab'"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive/')\n",
    "\n",
    "rootDir = '/content/drive/MyDrive/'\n",
    "\n",
    "TIMESTAMP = '2022_03_30'\n",
    "\n",
    "dataPath = rootDir + 'TP2/Datasets/Recipe1M/' + TIMESTAMP\n",
    "tarPath = rootDir + 'Colab Notebooks/recime/data/' + TIMESTAMP\n",
    "weightDir = rootDir + 'Colab Notebooks/recime/weights/'\n",
    "logDir = rootDir + 'Colab Notebooks/recime/logs/titleTrainer/'\n",
    "rayDir = rootDir + 'Colab Notebooks/recime/ray/titleTrainer/'\n",
    "\n",
    "\n",
    "import sys\n",
    "sys.path.append(rootDir + 'Colab Notebooks/recime/LSTM')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('D:/02_Studium/SBX/mad-recime/network/LSTM')\n",
    "\n",
    "tarPath = 'D:/02_Studium/SBX/mad-recime/data/2022_03_19'\n",
    "logDir = 'D:/02_Studium/SBX/mad-recime/network/LSTM/logs/pl/'\n",
    "rayDir = 'D:/02_Studium/SBX/mad-recime/network/LSTM/ray/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "from pytorch_lightning.callbacks import Callback\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from pytorch_lightning.utilities.seed import seed_everything\n",
    "from pytorch_lightning import loggers as pl_loggers\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import re\n",
    "from ray import tune\n",
    "from ray.tune import CLIReporter\n",
    "from ray.tune.schedulers import ASHAScheduler\n",
    "from ray.tune.integration.pytorch_lightning import TuneReportCallback,  TuneReportCheckpointCallback\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import src.nlp_utils.model as models\n",
    "import src.nlp_utils.data_module as data_module\n",
    "\n",
    "import importlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "available_gpus = [torch.cuda.device(i) for i in range(torch.cuda.device_count())]\n",
    "num_gpus = len(available_gpus)\n",
    "num_gpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set seed to get consistent results, deactivate if random results are wanted\n",
    "seed_everything(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\02_Studium/SBX/mad-recime/network/LSTM\\src\\nlp_utils\\preProc.py:38: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  baseFrame = baseFrame.append(pd.read_pickle(dataSetSplits[split]))\n",
      "D:\\02_Studium/SBX/mad-recime/network/LSTM\\src\\nlp_utils\\preProc.py:38: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  baseFrame = baseFrame.append(pd.read_pickle(dataSetSplits[split]))\n"
     ]
    }
   ],
   "source": [
    "importlib.reload(data_module)\n",
    "\n",
    "titleMod = data_module.TitleDataModule(tarPath, setSize=8)\n",
    "titleMod.setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(models)\n",
    "\n",
    "from src.nlp_utils.config import create_config\n",
    "\n",
    "tb_logger = pl_loggers.TensorBoardLogger(save_dir=logDir)\n",
    "\n",
    "config = create_config({})\n",
    "\n",
    "config['vocabSize'] = titleMod.vocab_size\n",
    "\n",
    "model = models.EmbedLSTM(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "C:\\Users\\Hannes\\miniconda3\\envs\\madEnv\\lib\\site-packages\\pytorch_lightning\\core\\datamodule.py:469: LightningDeprecationWarning: DataModule.setup has already been called, so it will not be called again. In v1.6 this behavior will change to always call DataModule.setup.\n",
      "  rank_zero_deprecation(\n",
      "\n",
      "  | Name      | Type             | Params\n",
      "-----------------------------------------------\n",
      "0 | embed     | Embedding        | 171 K \n",
      "1 | lstm      | LSTM             | 571 K \n",
      "2 | linear    | Linear           | 147 K \n",
      "3 | criterion | CrossEntropyLoss | 0     \n",
      "-----------------------------------------------\n",
      "889 K     Trainable params\n",
      "0         Non-trainable params\n",
      "889 K     Total params\n",
      "3.560     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                      "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "C:\\Users\\Hannes\\miniconda3\\envs\\madEnv\\lib\\site-packages\\pytorch_lightning\\trainer\\data_loading.py:432: UserWarning: The number of training samples (7) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|          | 0/10 [00:00<?, ?it/s] "
     ]
    }
   ],
   "source": [
    "trainer = Trainer(gpus=num_gpus, max_epochs=config['epochs'], logger=tb_logger)\n",
    "trainer.fit(model, titleMod)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mnist = MNISTDataModule(my_path)\n",
    "# model = LitClassifier()\n",
    "\n",
    "# trainer = Trainer()\n",
    "# trainer.fit(model, mnist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stop_callback = EarlyStopping(\n",
    "    monitor=\"val_loss\",\n",
    "    min_delta=0.05,\n",
    "    patience=3,\n",
    "    verbose=False,\n",
    "    mode=\"min\",\n",
    "    divergence_threshold=3.00,\n",
    ")\n",
    "\n",
    "class MyPrintingCallback(Callback):\n",
    "    def on_fit_start(self, trainer, pl_module):\n",
    "        print(\"Starting to train!\")\n",
    "\n",
    "    def on_fit_end(self, trainer, pl_module):\n",
    "        print(\"Finished training\")\n",
    "\n",
    "    def on_test_start(self, trainer, pl_module):\n",
    "        print(\"Start to test\")\n",
    "\n",
    "    def on_test_end(self, trainer, pl_module):\n",
    "        print(\"Finished testing\")\n",
    "\n",
    "\n",
    "# checkpoint_callback = ModelCheckpoint(\n",
    "#     monitor=\"val_epoch_stance_F1\",\n",
    "#     filename=\"{epoch}-{val_loss:.2f}-{val_epoch_stance_F1:.2f}\",\n",
    "#     save_top_k=3,\n",
    "#     mode=\"max\",\n",
    "# )\n",
    "\n",
    "callback = TuneReportCallback(\n",
    "    {\"loss\": \"val_loss\"}, on=\"validation_end\"\n",
    ")\n",
    "\n",
    "\n",
    "# training loop that tests out different hyperparameters and saves the results into the log folder\n",
    "def train_tune(config, callbacks, epochs=100):#, gpus=num_gpus):\n",
    "    dataModule = data_module.TitleDataModule(tarPath, setSize=8, num_workers=2, config=config)\n",
    "    dataModule.setup()\n",
    "    \n",
    "    config[\"vocabSize\"] = dataModule.vocab_size\n",
    "    \n",
    "    model = models.EmbedLSTM(config)\n",
    "\n",
    "    trainer = pl.Trainer(\n",
    "        # gpus=gpus,\n",
    "        log_every_n_steps=1,\n",
    "        flush_logs_every_n_steps=1,\n",
    "        callbacks=callbacks,\n",
    "        deterministic=True,\n",
    "        default_root_dir=logDir,\n",
    "        max_epochs=epochs,\n",
    "    )  \n",
    "\n",
    "    trainer.fit(model, datamodule=dataModule)\n",
    "\n",
    "    # might not be called due to scheduler and reporter which cancel training early if results don't look promising\n",
    "    # trainer.test(model, datamodule=dataModule)\n",
    "\n",
    "\n",
    "# config with radom sampling for learning rate and batch size\n",
    "config = {\n",
    "    'lr': tune.sample_from(lambda: 10 ** np.random.uniform(-5, -3)),\n",
    "    'batchSize': tune.choice([1, 8, 16, 32, 64, 128]),\n",
    "    'hiddenSize': tune.choice([16, 32, 64, 128, 256]),\n",
    "    'epochs': 30,\n",
    "    'num_trials': 5,\n",
    "}\n",
    "\n",
    "\n",
    "# callbacks = [MyPrintingCallback(), early_stop_callback]\n",
    "callbacks = [callback]\n",
    "\n",
    "scheduler = ASHAScheduler(max_t=config['epochs'], grace_period=1, reduction_factor=2)\n",
    "\n",
    "reporter = CLIReporter(\n",
    "    parameter_columns=['lr', 'batchSize', 'hiddenSize'],\n",
    "    metric_columns=['loss'],\n",
    ")\n",
    "\n",
    "# ray.init(local_mode=True, num_cpus=4, num_gpus=0)  # for debugging\n",
    "\n",
    "# create versioning for multiple runs\n",
    "# def atoi(text):\n",
    "#     return int(text) if text.isdigit() else text\n",
    "\n",
    "\n",
    "# def natural_keys(text):\n",
    "#     return [atoi(c) for c in re.split(\"(\\d+)\", text)]\n",
    "\n",
    "\n",
    "# log_dir = \"../logs/StancePrediction_SemEval/lightning_logs/\"\n",
    "# log_path = os.path.join(path, log_dir)\n",
    "# os.makedirs(os.path.dirname(log_path), exist_ok=True)\n",
    "# ver = os.listdir(os.path.join(path, log_dir))\n",
    "# ver.sort(key=natural_keys)\n",
    "# if ver:\n",
    "#     version = int(ver[-1].split(\"_\", 2)[-1]) + 1\n",
    "# else:\n",
    "#     version = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-01 22:51:01,920\tWARNING tune.py:583 -- Tune detects GPUs, but no trials are using GPUs. To enable trials to use GPUs, set tune.run(resources_per_trial={'gpu': 1}...) which allows Tune to expose 1 GPU to each trial. You can also override `Trainable.default_resource_request` if using the Trainable API.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Current time: 2022-04-01 22:51:02 (running for 00:00:00.62)\n",
      "Memory usage on this node: 7.2/7.9 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.\n",
      "Using AsyncHyperBand: num_stopped=0\n",
      "Bracket: Iter 16.000: None | Iter 8.000: None | Iter 4.000: None | Iter 2.000: None | Iter 1.000: None\n",
      "Resources requested: 0/4 CPUs, 0/1 GPUs, 0.0/0.6 GiB heap, 0.0/0.3 GiB objects\n",
      "Result logdir: D:\\02_Studium\\SBX\\mad-recime\\network\\LSTM\\ray\\version_ReciMe\n",
      "Number of trials: 5/5 (5 PENDING)\n",
      "+------------------------+----------+-------+-------------+-------------+--------------+\n",
      "| Trial name             | status   | loc   |          lr |   batchSize |   hiddenSize |\n",
      "|------------------------+----------+-------+-------------+-------------+--------------|\n",
      "| train_tune_70cbe_00000 | PENDING  |       | 4.0697e-05  |         128 |           64 |\n",
      "| train_tune_70cbe_00001 | PENDING  |       | 2.78109e-05 |          16 |           32 |\n",
      "| train_tune_70cbe_00002 | PENDING  |       | 0.000768811 |         128 |           16 |\n",
      "| train_tune_70cbe_00003 | PENDING  |       | 1.34829e-05 |           8 |          128 |\n",
      "| train_tune_70cbe_00004 | PENDING  |       | 0.00024695  |          16 |           16 |\n",
      "+------------------------+----------+-------+-------------+-------------+--------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-04-01 22:51:07 (running for 00:00:05.63)\n",
      "Memory usage on this node: 7.4/7.9 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.\n",
      "Using AsyncHyperBand: num_stopped=0\n",
      "Bracket: Iter 16.000: None | Iter 8.000: None | Iter 4.000: None | Iter 2.000: None | Iter 1.000: None\n",
      "Resources requested: 0/4 CPUs, 0/1 GPUs, 0.0/0.6 GiB heap, 0.0/0.3 GiB objects\n",
      "Result logdir: D:\\02_Studium\\SBX\\mad-recime\\network\\LSTM\\ray\\version_ReciMe\n",
      "Number of trials: 5/5 (5 PENDING)\n",
      "+------------------------+----------+-------+-------------+-------------+--------------+\n",
      "| Trial name             | status   | loc   |          lr |   batchSize |   hiddenSize |\n",
      "|------------------------+----------+-------+-------------+-------------+--------------|\n",
      "| train_tune_70cbe_00000 | PENDING  |       | 4.0697e-05  |         128 |           64 |\n",
      "| train_tune_70cbe_00001 | PENDING  |       | 2.78109e-05 |          16 |           32 |\n",
      "| train_tune_70cbe_00002 | PENDING  |       | 0.000768811 |         128 |           16 |\n",
      "| train_tune_70cbe_00003 | PENDING  |       | 1.34829e-05 |           8 |          128 |\n",
      "| train_tune_70cbe_00004 | PENDING  |       | 0.00024695  |          16 |           16 |\n",
      "+------------------------+----------+-------+-------------+-------------+--------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-04-01 22:51:12 (running for 00:00:10.70)\n",
      "Memory usage on this node: 7.2/7.9 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.\n",
      "Using AsyncHyperBand: num_stopped=0\n",
      "Bracket: Iter 16.000: None | Iter 8.000: None | Iter 4.000: None | Iter 2.000: None | Iter 1.000: None\n",
      "Resources requested: 0/4 CPUs, 0/1 GPUs, 0.0/0.6 GiB heap, 0.0/0.3 GiB objects\n",
      "Result logdir: D:\\02_Studium\\SBX\\mad-recime\\network\\LSTM\\ray\\version_ReciMe\n",
      "Number of trials: 5/5 (5 PENDING)\n",
      "+------------------------+----------+-------+-------------+-------------+--------------+\n",
      "| Trial name             | status   | loc   |          lr |   batchSize |   hiddenSize |\n",
      "|------------------------+----------+-------+-------------+-------------+--------------|\n",
      "| train_tune_70cbe_00000 | PENDING  |       | 4.0697e-05  |         128 |           64 |\n",
      "| train_tune_70cbe_00001 | PENDING  |       | 2.78109e-05 |          16 |           32 |\n",
      "| train_tune_70cbe_00002 | PENDING  |       | 0.000768811 |         128 |           16 |\n",
      "| train_tune_70cbe_00003 | PENDING  |       | 1.34829e-05 |           8 |          128 |\n",
      "| train_tune_70cbe_00004 | PENDING  |       | 0.00024695  |          16 |           16 |\n",
      "+------------------------+----------+-------+-------------+-------------+--------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-01 22:51:17,089\tWARNING util.py:163 -- The `start_trial` operation took 0.596 s, which may be a performance bottleneck.\n",
      "2022-04-01 22:51:23,520\tWARNING worker.py:1326 -- Traceback (most recent call last):\n",
      "  File \"python\\ray\\_raylet.pyx\", line 633, in ray._raylet.execute_task\n",
      "  File \"python\\ray\\_raylet.pyx\", line 674, in ray._raylet.execute_task\n",
      "  File \"python\\ray\\_raylet.pyx\", line 640, in ray._raylet.execute_task\n",
      "  File \"python\\ray\\_raylet.pyx\", line 644, in ray._raylet.execute_task\n",
      "  File \"python\\ray\\_raylet.pyx\", line 593, in ray._raylet.execute_task.function_executor\n",
      "  File \"C:\\Users\\Hannes\\miniconda3\\envs\\madEnv\\lib\\site-packages\\ray\\_private\\function_manager.py\", line 648, in actor_method_executor\n",
      "    return method(__ray_actor, *args, **kwargs)\n",
      "  File \"C:\\Users\\Hannes\\miniconda3\\envs\\madEnv\\lib\\site-packages\\ray\\_private\\function_manager.py\", line 544, in temporary_actor_method\n",
      "    raise RuntimeError(\n",
      "RuntimeError: The actor with name ImplicitFunc failed to import on the worker. This may be because needed library dependencies are not installed in the worker environment:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Hannes\\miniconda3\\envs\\madEnv\\lib\\site-packages\\ray\\_private\\function_manager.py\", line 603, in _load_actor_class_from_gcs\n",
      "    actor_class = pickle.loads(pickled_class)\n",
      "  File \"C:\\Users\\Hannes\\miniconda3\\envs\\madEnv\\lib\\site-packages\\ray\\cloudpickle\\cloudpickle.py\", line 679, in subimport\n",
      "    __import__(name)\n",
      "ModuleNotFoundError: No module named 'src'\n",
      "\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"python\\ray\\_raylet.pyx\", line 774, in ray._raylet.task_execution_handler\n",
      "  File \"python\\ray\\_raylet.pyx\", line 595, in ray._raylet.execute_task\n",
      "  File \"python\\ray\\_raylet.pyx\", line 715, in ray._raylet.execute_task\n",
      "  File \"C:\\Users\\Hannes\\miniconda3\\envs\\madEnv\\lib\\site-packages\\ray\\_private\\function_manager.py\", line 544, in temporary_actor_method\n",
      "    raise RuntimeError(\n",
      "RuntimeError: The actor with name ImplicitFunc failed to import on the worker. This may be because needed library dependencies are not installed in the worker environment:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Hannes\\miniconda3\\envs\\madEnv\\lib\\site-packages\\ray\\_private\\function_manager.py\", line 603, in _load_actor_class_from_gcs\n",
      "    actor_class = pickle.loads(pickled_class)\n",
      "  File \"C:\\Users\\Hannes\\miniconda3\\envs\\madEnv\\lib\\site-packages\\ray\\cloudpickle\\cloudpickle.py\", line 679, in subimport\n",
      "    __import__(name)\n",
      "ModuleNotFoundError: No module named 'src'\n",
      "\n",
      "An unexpected internal error occurred while the worker was executing a task.\n",
      " pid=2276)\u001b[0m 2022-04-01 22:51:23,341\tERROR worker.py:430 -- SystemExit was raised from the worker.\n",
      " pid=2276)\u001b[0m Traceback (most recent call last):\n",
      " pid=2276)\u001b[0m   File \"python\\ray\\_raylet.pyx\", line 633, in ray._raylet.execute_task\n",
      " pid=2276)\u001b[0m   File \"python\\ray\\_raylet.pyx\", line 674, in ray._raylet.execute_task\n",
      " pid=2276)\u001b[0m   File \"python\\ray\\_raylet.pyx\", line 640, in ray._raylet.execute_task\n",
      " pid=2276)\u001b[0m   File \"python\\ray\\_raylet.pyx\", line 644, in ray._raylet.execute_task\n",
      " pid=2276)\u001b[0m   File \"python\\ray\\_raylet.pyx\", line 593, in ray._raylet.execute_task.function_executor\n",
      " pid=2276)\u001b[0m   File \"C:\\Users\\Hannes\\miniconda3\\envs\\madEnv\\lib\\site-packages\\ray\\_private\\function_manager.py\", line 648, in actor_method_executor\n",
      " pid=2276)\u001b[0m     return method(__ray_actor, *args, **kwargs)\n",
      " pid=2276)\u001b[0m   File \"C:\\Users\\Hannes\\miniconda3\\envs\\madEnv\\lib\\site-packages\\ray\\_private\\function_manager.py\", line 544, in temporary_actor_method\n",
      " pid=2276)\u001b[0m     raise RuntimeError(\n",
      " pid=2276)\u001b[0m RuntimeError: The actor with name ImplicitFunc failed to import on the worker. This may be because needed library dependencies are not installed in the worker environment:\n",
      " pid=2276)\u001b[0m raryActor\n",
      " pid=2276)\u001b[0m Traceback (most recent call last):\n",
      " pid=2276)\u001b[0m   File \"C:\\Users\\Hannes\\miniconda3\\envs\\madEnv\\lib\\site-packages\\ray\\_private\\function_manager.py\", line 603, in _load_actor_class_from_gcs\n",
      " pid=2276)\u001b[0m     actor_class = pickle.loads(pickled_class)\n",
      " pid=2276)\u001b[0m   File \"C:\\Users\\Hannes\\miniconda3\\envs\\madEnv\\lib\\site-packages\\ray\\cloudpickle\\cloudpickle.py\", line 679, in subimport\n",
      " pid=2276)\u001b[0m     __import__(name)\n",
      " pid=2276)\u001b[0m ModuleNotFoundError: No module named 'src'\n",
      " pid=2276)\u001b[0m raryActor\n",
      " pid=2276)\u001b[0m raryActor\n",
      " pid=2276)\u001b[0m During handling of the above exception, another exception occurred:\n",
      " pid=2276)\u001b[0m raryActor\n",
      " pid=2276)\u001b[0m Traceback (most recent call last):\n",
      " pid=2276)\u001b[0m   File \"python\\ray\\_raylet.pyx\", line 774, in ray._raylet.task_execution_handler\n",
      " pid=2276)\u001b[0m   File \"python\\ray\\_raylet.pyx\", line 595, in ray._raylet.execute_task\n",
      " pid=2276)\u001b[0m   File \"python\\ray\\_raylet.pyx\", line 715, in ray._raylet.execute_task\n",
      " pid=2276)\u001b[0m   File \"C:\\Users\\Hannes\\miniconda3\\envs\\madEnv\\lib\\site-packages\\ray\\_private\\function_manager.py\", line 544, in temporary_actor_method\n",
      " pid=2276)\u001b[0m     raise RuntimeError(\n",
      " pid=2276)\u001b[0m RuntimeError: The actor with name ImplicitFunc failed to import on the worker. This may be because needed library dependencies are not installed in the worker environment:\n",
      " pid=2276)\u001b[0m raryActor\n",
      " pid=2276)\u001b[0m Traceback (most recent call last):\n",
      " pid=2276)\u001b[0m   File \"C:\\Users\\Hannes\\miniconda3\\envs\\madEnv\\lib\\site-packages\\ray\\_private\\function_manager.py\", line 603, in _load_actor_class_from_gcs\n",
      " pid=2276)\u001b[0m     actor_class = pickle.loads(pickled_class)\n",
      " pid=2276)\u001b[0m   File \"C:\\Users\\Hannes\\miniconda3\\envs\\madEnv\\lib\\site-packages\\ray\\cloudpickle\\cloudpickle.py\", line 679, in subimport\n",
      " pid=2276)\u001b[0m     __import__(name)\n",
      " pid=2276)\u001b[0m ModuleNotFoundError: No module named 'src'\n",
      " pid=2276)\u001b[0m raryActor\n",
      " pid=2276)\u001b[0m raryActor\n",
      " pid=2276)\u001b[0m During handling of the above exception, another exception occurred:\n",
      " pid=2276)\u001b[0m raryActor\n",
      " pid=2276)\u001b[0m Traceback (most recent call last):\n",
      " pid=2276)\u001b[0m   File \"python\\ray\\_raylet.pyx\", line 803, in ray._raylet.task_execution_handler\n",
      " pid=2276)\u001b[0m SystemExit\n",
      "2022-04-01 22:51:23,940\tWARNING worker.py:1326 -- A worker died or was killed while executing a task by an unexpected system error. To troubleshoot the problem, check the logs for the dead worker. RayTask ID: ffffffffffffffff9de1b90bcde8cdc4b093f82201000000 Worker ID: 9cbdd7cf4ae23e7db602ebeee4c23a8836772b87a32c23728d334415 Node ID: fb972aa1896eeec4945941e547fa10f3a552dffd6291b1f357977c0b Worker IP address: 127.0.0.1 Worker port: 65072 Worker PID: 2276\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Current time: 2022-04-01 22:51:18 (running for 00:00:16.22)\n",
      "Memory usage on this node: 7.4/7.9 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.\n",
      "Using AsyncHyperBand: num_stopped=0\n",
      "Bracket: Iter 16.000: None | Iter 8.000: None | Iter 4.000: None | Iter 2.000: None | Iter 1.000: None\n",
      "Resources requested: 1.0/4 CPUs, 0/1 GPUs, 0.0/0.6 GiB heap, 0.0/0.3 GiB objects\n",
      "Result logdir: D:\\02_Studium\\SBX\\mad-recime\\network\\LSTM\\ray\\version_ReciMe\n",
      "Number of trials: 5/5 (4 PENDING, 1 RUNNING)\n",
      "+------------------------+----------+-------+-------------+-------------+--------------+\n",
      "| Trial name             | status   | loc   |          lr |   batchSize |   hiddenSize |\n",
      "|------------------------+----------+-------+-------------+-------------+--------------|\n",
      "| train_tune_70cbe_00000 | RUNNING  |       | 4.0697e-05  |         128 |           64 |\n",
      "| train_tune_70cbe_00001 | PENDING  |       | 2.78109e-05 |          16 |           32 |\n",
      "| train_tune_70cbe_00002 | PENDING  |       | 0.000768811 |         128 |           16 |\n",
      "| train_tune_70cbe_00003 | PENDING  |       | 1.34829e-05 |           8 |          128 |\n",
      "| train_tune_70cbe_00004 | PENDING  |       | 0.00024695  |          16 |           16 |\n",
      "+------------------------+----------+-------+-------------+-------------+--------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-01 22:51:24,554\tWARNING util.py:163 -- The `on_step_begin` operation took 0.515 s, which may be a performance bottleneck.\n",
      "2022-04-01 22:51:24,706\tERROR trial_runner.py:920 -- Trial train_tune_70cbe_00000: Error processing event.\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Hannes\\miniconda3\\envs\\madEnv\\lib\\site-packages\\ray\\tune\\trial_runner.py\", line 886, in _process_trial\n",
      "    results = self.trial_executor.fetch_result(trial)\n",
      "  File \"C:\\Users\\Hannes\\miniconda3\\envs\\madEnv\\lib\\site-packages\\ray\\tune\\ray_trial_executor.py\", line 675, in fetch_result\n",
      "    result = ray.get(trial_future[0], timeout=DEFAULT_GET_TIMEOUT)\n",
      "  File \"C:\\Users\\Hannes\\miniconda3\\envs\\madEnv\\lib\\site-packages\\ray\\_private\\client_mode_hook.py\", line 105, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"C:\\Users\\Hannes\\miniconda3\\envs\\madEnv\\lib\\site-packages\\ray\\worker.py\", line 1765, in get\n",
      "    raise value\n",
      "ray.exceptions.RayActorError: The actor died unexpectedly before finishing this task.\n",
      "\tclass_name: ImplicitFunc\n",
      "\tactor_id: 9de1b90bcde8cdc4b093f82201000000\n",
      "\tpid: 2276\n",
      "\tnamespace: dfa27098-ba22-48a7-a645-ed4d5ff7d961\n",
      "\tip: 127.0.0.1\n",
      "The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR_EXIT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for train_tune_70cbe_00000:\n",
      "  trial_id: 70cbe_00000\n",
      "  \n",
      "== Status ==\n",
      "Current time: 2022-04-01 22:51:24 (running for 00:00:22.96)\n",
      "Memory usage on this node: 7.4/7.9 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.\n",
      "Using AsyncHyperBand: num_stopped=0\n",
      "Bracket: Iter 16.000: None | Iter 8.000: None | Iter 4.000: None | Iter 2.000: None | Iter 1.000: None\n",
      "Resources requested: 0/4 CPUs, 0/1 GPUs, 0.0/0.6 GiB heap, 0.0/0.3 GiB objects\n",
      "Result logdir: D:\\02_Studium\\SBX\\mad-recime\\network\\LSTM\\ray\\version_ReciMe\n",
      "Number of trials: 5/5 (1 ERROR, 4 PENDING)\n",
      "+------------------------+----------+-------+-------------+-------------+--------------+\n",
      "| Trial name             | status   | loc   |          lr |   batchSize |   hiddenSize |\n",
      "|------------------------+----------+-------+-------------+-------------+--------------|\n",
      "| train_tune_70cbe_00001 | PENDING  |       | 2.78109e-05 |          16 |           32 |\n",
      "| train_tune_70cbe_00002 | PENDING  |       | 0.000768811 |         128 |           16 |\n",
      "| train_tune_70cbe_00003 | PENDING  |       | 1.34829e-05 |           8 |          128 |\n",
      "| train_tune_70cbe_00004 | PENDING  |       | 0.00024695  |          16 |           16 |\n",
      "| train_tune_70cbe_00000 | ERROR    |       | 4.0697e-05  |         128 |           64 |\n",
      "+------------------------+----------+-------+-------------+-------------+--------------+\n",
      "Number of errored trials: 1\n",
      "+------------------------+--------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "| Trial name             |   # failures | error file                                                                                                                                                    |\n",
      "|------------------------+--------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
      "| train_tune_70cbe_00000 |            1 | D:\\02_Studium\\SBX\\mad-recime\\network\\LSTM\\ray\\version_ReciMe\\train_tune_70cbe_00000_0_batchSize=128,hiddenSize=64,lr=4.0697e-05_2022-04-01_22-51-02\\error.txt |\n",
      "+------------------------+--------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-01 22:51:26,161\tWARNING util.py:163 -- The `start_trial` operation took 0.507 s, which may be a performance bottleneck.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Current time: 2022-04-01 22:51:29 (running for 00:00:27.99)\n",
      "Memory usage on this node: 7.3/7.9 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.\n",
      "Using AsyncHyperBand: num_stopped=0\n",
      "Bracket: Iter 16.000: None | Iter 8.000: None | Iter 4.000: None | Iter 2.000: None | Iter 1.000: None\n",
      "Resources requested: 0/4 CPUs, 0/1 GPUs, 0.0/0.6 GiB heap, 0.0/0.3 GiB objects\n",
      "Result logdir: D:\\02_Studium\\SBX\\mad-recime\\network\\LSTM\\ray\\version_ReciMe\n",
      "Number of trials: 5/5 (1 ERROR, 4 PENDING)\n",
      "+------------------------+----------+-------+-------------+-------------+--------------+\n",
      "| Trial name             | status   | loc   |          lr |   batchSize |   hiddenSize |\n",
      "|------------------------+----------+-------+-------------+-------------+--------------|\n",
      "| train_tune_70cbe_00001 | PENDING  |       | 2.78109e-05 |          16 |           32 |\n",
      "| train_tune_70cbe_00002 | PENDING  |       | 0.000768811 |         128 |           16 |\n",
      "| train_tune_70cbe_00003 | PENDING  |       | 1.34829e-05 |           8 |          128 |\n",
      "| train_tune_70cbe_00004 | PENDING  |       | 0.00024695  |          16 |           16 |\n",
      "| train_tune_70cbe_00000 | ERROR    |       | 4.0697e-05  |         128 |           64 |\n",
      "+------------------------+----------+-------+-------------+-------------+--------------+\n",
      "Number of errored trials: 1\n",
      "+------------------------+--------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "| Trial name             |   # failures | error file                                                                                                                                                    |\n",
      "|------------------------+--------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
      "| train_tune_70cbe_00000 |            1 | D:\\02_Studium\\SBX\\mad-recime\\network\\LSTM\\ray\\version_ReciMe\\train_tune_70cbe_00000_0_batchSize=128,hiddenSize=64,lr=4.0697e-05_2022-04-01_22-51-02\\error.txt |\n",
      "+------------------------+--------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=)\u001b[0m [2022-04-01 22:51:32,449 E 21372 2644] (raylet.exe) worker_pool.cc:481: Some workers of the worker process(11408) have not registered within the timeout. The process is still alive, probably it's hanging during start.\n",
      "\u001b[2m\u001b[36m(pid=)\u001b[0m [2022-04-01 22:51:32,461 E 21372 2644] (raylet.exe) worker_pool.cc:481: Some workers of the worker process(7668) have not registered within the timeout. The process is still alive, probably it's hanging during start.\n",
      "\u001b[2m\u001b[36m(pid=)\u001b[0m [2022-04-01 22:51:32,608 E 21372 2644] (raylet.exe) worker_pool.cc:481: Some workers of the worker process(10724) have not registered within the timeout. The process is still alive, probably it's hanging during start.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Current time: 2022-04-01 22:51:34 (running for 00:00:33.04)\n",
      "Memory usage on this node: 7.3/7.9 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.\n",
      "Using AsyncHyperBand: num_stopped=0\n",
      "Bracket: Iter 16.000: None | Iter 8.000: None | Iter 4.000: None | Iter 2.000: None | Iter 1.000: None\n",
      "Resources requested: 0/4 CPUs, 0/1 GPUs, 0.0/0.6 GiB heap, 0.0/0.3 GiB objects\n",
      "Result logdir: D:\\02_Studium\\SBX\\mad-recime\\network\\LSTM\\ray\\version_ReciMe\n",
      "Number of trials: 5/5 (1 ERROR, 4 PENDING)\n",
      "+------------------------+----------+-------+-------------+-------------+--------------+\n",
      "| Trial name             | status   | loc   |          lr |   batchSize |   hiddenSize |\n",
      "|------------------------+----------+-------+-------------+-------------+--------------|\n",
      "| train_tune_70cbe_00001 | PENDING  |       | 2.78109e-05 |          16 |           32 |\n",
      "| train_tune_70cbe_00002 | PENDING  |       | 0.000768811 |         128 |           16 |\n",
      "| train_tune_70cbe_00003 | PENDING  |       | 1.34829e-05 |           8 |          128 |\n",
      "| train_tune_70cbe_00004 | PENDING  |       | 0.00024695  |          16 |           16 |\n",
      "| train_tune_70cbe_00000 | ERROR    |       | 4.0697e-05  |         128 |           64 |\n",
      "+------------------------+----------+-------+-------------+-------------+--------------+\n",
      "Number of errored trials: 1\n",
      "+------------------------+--------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "| Trial name             |   # failures | error file                                                                                                                                                    |\n",
      "|------------------------+--------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
      "| train_tune_70cbe_00000 |            1 | D:\\02_Studium\\SBX\\mad-recime\\network\\LSTM\\ray\\version_ReciMe\\train_tune_70cbe_00000_0_batchSize=128,hiddenSize=64,lr=4.0697e-05_2022-04-01_22-51-02\\error.txt |\n",
      "+------------------------+--------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-04-01 22:51:40 (running for 00:00:38.08)\n",
      "Memory usage on this node: 7.4/7.9 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.\n",
      "Using AsyncHyperBand: num_stopped=0\n",
      "Bracket: Iter 16.000: None | Iter 8.000: None | Iter 4.000: None | Iter 2.000: None | Iter 1.000: None\n",
      "Resources requested: 0/4 CPUs, 0/1 GPUs, 0.0/0.6 GiB heap, 0.0/0.3 GiB objects\n",
      "Result logdir: D:\\02_Studium\\SBX\\mad-recime\\network\\LSTM\\ray\\version_ReciMe\n",
      "Number of trials: 5/5 (1 ERROR, 4 PENDING)\n",
      "+------------------------+----------+-------+-------------+-------------+--------------+\n",
      "| Trial name             | status   | loc   |          lr |   batchSize |   hiddenSize |\n",
      "|------------------------+----------+-------+-------------+-------------+--------------|\n",
      "| train_tune_70cbe_00001 | PENDING  |       | 2.78109e-05 |          16 |           32 |\n",
      "| train_tune_70cbe_00002 | PENDING  |       | 0.000768811 |         128 |           16 |\n",
      "| train_tune_70cbe_00003 | PENDING  |       | 1.34829e-05 |           8 |          128 |\n",
      "| train_tune_70cbe_00004 | PENDING  |       | 0.00024695  |          16 |           16 |\n",
      "| train_tune_70cbe_00000 | ERROR    |       | 4.0697e-05  |         128 |           64 |\n",
      "+------------------------+----------+-------+-------------+-------------+--------------+\n",
      "Number of errored trials: 1\n",
      "+------------------------+--------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "| Trial name             |   # failures | error file                                                                                                                                                    |\n",
      "|------------------------+--------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
      "| train_tune_70cbe_00000 |            1 | D:\\02_Studium\\SBX\\mad-recime\\network\\LSTM\\ray\\version_ReciMe\\train_tune_70cbe_00000_0_batchSize=128,hiddenSize=64,lr=4.0697e-05_2022-04-01_22-51-02\\error.txt |\n",
      "+------------------------+--------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-04-01 22:51:45 (running for 00:00:43.34)\n",
      "Memory usage on this node: 7.3/7.9 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.\n",
      "Using AsyncHyperBand: num_stopped=0\n",
      "Bracket: Iter 16.000: None | Iter 8.000: None | Iter 4.000: None | Iter 2.000: None | Iter 1.000: None\n",
      "Resources requested: 0/4 CPUs, 0/1 GPUs, 0.0/0.6 GiB heap, 0.0/0.3 GiB objects\n",
      "Result logdir: D:\\02_Studium\\SBX\\mad-recime\\network\\LSTM\\ray\\version_ReciMe\n",
      "Number of trials: 5/5 (1 ERROR, 4 PENDING)\n",
      "+------------------------+----------+-------+-------------+-------------+--------------+\n",
      "| Trial name             | status   | loc   |          lr |   batchSize |   hiddenSize |\n",
      "|------------------------+----------+-------+-------------+-------------+--------------|\n",
      "| train_tune_70cbe_00001 | PENDING  |       | 2.78109e-05 |          16 |           32 |\n",
      "| train_tune_70cbe_00002 | PENDING  |       | 0.000768811 |         128 |           16 |\n",
      "| train_tune_70cbe_00003 | PENDING  |       | 1.34829e-05 |           8 |          128 |\n",
      "| train_tune_70cbe_00004 | PENDING  |       | 0.00024695  |          16 |           16 |\n",
      "| train_tune_70cbe_00000 | ERROR    |       | 4.0697e-05  |         128 |           64 |\n",
      "+------------------------+----------+-------+-------------+-------------+--------------+\n",
      "Number of errored trials: 1\n",
      "+------------------------+--------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "| Trial name             |   # failures | error file                                                                                                                                                    |\n",
      "|------------------------+--------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
      "| train_tune_70cbe_00000 |            1 | D:\\02_Studium\\SBX\\mad-recime\\network\\LSTM\\ray\\version_ReciMe\\train_tune_70cbe_00000_0_batchSize=128,hiddenSize=64,lr=4.0697e-05_2022-04-01_22-51-02\\error.txt |\n",
      "+------------------------+--------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-01 22:51:47,330\tWARNING util.py:163 -- The `start_trial` operation took 0.517 s, which may be a performance bottleneck.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Current time: 2022-04-01 22:51:50 (running for 00:00:48.39)\n",
      "Memory usage on this node: 7.4/7.9 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.\n",
      "Using AsyncHyperBand: num_stopped=0\n",
      "Bracket: Iter 16.000: None | Iter 8.000: None | Iter 4.000: None | Iter 2.000: None | Iter 1.000: None\n",
      "Resources requested: 0/4 CPUs, 0/1 GPUs, 0.0/0.6 GiB heap, 0.0/0.3 GiB objects\n",
      "Result logdir: D:\\02_Studium\\SBX\\mad-recime\\network\\LSTM\\ray\\version_ReciMe\n",
      "Number of trials: 5/5 (1 ERROR, 4 PENDING)\n",
      "+------------------------+----------+-------+-------------+-------------+--------------+\n",
      "| Trial name             | status   | loc   |          lr |   batchSize |   hiddenSize |\n",
      "|------------------------+----------+-------+-------------+-------------+--------------|\n",
      "| train_tune_70cbe_00001 | PENDING  |       | 2.78109e-05 |          16 |           32 |\n",
      "| train_tune_70cbe_00002 | PENDING  |       | 0.000768811 |         128 |           16 |\n",
      "| train_tune_70cbe_00003 | PENDING  |       | 1.34829e-05 |           8 |          128 |\n",
      "| train_tune_70cbe_00004 | PENDING  |       | 0.00024695  |          16 |           16 |\n",
      "| train_tune_70cbe_00000 | ERROR    |       | 4.0697e-05  |         128 |           64 |\n",
      "+------------------------+----------+-------+-------------+-------------+--------------+\n",
      "Number of errored trials: 1\n",
      "+------------------------+--------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "| Trial name             |   # failures | error file                                                                                                                                                    |\n",
      "|------------------------+--------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
      "| train_tune_70cbe_00000 |            1 | D:\\02_Studium\\SBX\\mad-recime\\network\\LSTM\\ray\\version_ReciMe\\train_tune_70cbe_00000_0_batchSize=128,hiddenSize=64,lr=4.0697e-05_2022-04-01_22-51-02\\error.txt |\n",
      "+------------------------+--------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-04-01 22:51:55 (running for 00:00:53.40)\n",
      "Memory usage on this node: 7.4/7.9 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.\n",
      "Using AsyncHyperBand: num_stopped=0\n",
      "Bracket: Iter 16.000: None | Iter 8.000: None | Iter 4.000: None | Iter 2.000: None | Iter 1.000: None\n",
      "Resources requested: 0/4 CPUs, 0/1 GPUs, 0.0/0.6 GiB heap, 0.0/0.3 GiB objects\n",
      "Result logdir: D:\\02_Studium\\SBX\\mad-recime\\network\\LSTM\\ray\\version_ReciMe\n",
      "Number of trials: 5/5 (1 ERROR, 4 PENDING)\n",
      "+------------------------+----------+-------+-------------+-------------+--------------+\n",
      "| Trial name             | status   | loc   |          lr |   batchSize |   hiddenSize |\n",
      "|------------------------+----------+-------+-------------+-------------+--------------|\n",
      "| train_tune_70cbe_00001 | PENDING  |       | 2.78109e-05 |          16 |           32 |\n",
      "| train_tune_70cbe_00002 | PENDING  |       | 0.000768811 |         128 |           16 |\n",
      "| train_tune_70cbe_00003 | PENDING  |       | 1.34829e-05 |           8 |          128 |\n",
      "| train_tune_70cbe_00004 | PENDING  |       | 0.00024695  |          16 |           16 |\n",
      "| train_tune_70cbe_00000 | ERROR    |       | 4.0697e-05  |         128 |           64 |\n",
      "+------------------------+----------+-------+-------------+-------------+--------------+\n",
      "Number of errored trials: 1\n",
      "+------------------------+--------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "| Trial name             |   # failures | error file                                                                                                                                                    |\n",
      "|------------------------+--------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
      "| train_tune_70cbe_00000 |            1 | D:\\02_Studium\\SBX\\mad-recime\\network\\LSTM\\ray\\version_ReciMe\\train_tune_70cbe_00000_0_batchSize=128,hiddenSize=64,lr=4.0697e-05_2022-04-01_22-51-02\\error.txt |\n",
      "+------------------------+--------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-01 22:51:55,850\tWARNING util.py:163 -- The `on_step_begin` operation took 0.517 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(pid=)\u001b[0m [2022-04-01 22:51:58,036 E 21372 2644] (raylet.exe) worker_pool.cc:481: Some workers of the worker process(16448) have not registered within the timeout. The process is still alive, probably it's hanging during start.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Current time: 2022-04-01 22:52:00 (running for 00:00:58.41)\n",
      "Memory usage on this node: 7.3/7.9 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.\n",
      "Using AsyncHyperBand: num_stopped=0\n",
      "Bracket: Iter 16.000: None | Iter 8.000: None | Iter 4.000: None | Iter 2.000: None | Iter 1.000: None\n",
      "Resources requested: 0/4 CPUs, 0/1 GPUs, 0.0/0.6 GiB heap, 0.0/0.3 GiB objects\n",
      "Result logdir: D:\\02_Studium\\SBX\\mad-recime\\network\\LSTM\\ray\\version_ReciMe\n",
      "Number of trials: 5/5 (1 ERROR, 4 PENDING)\n",
      "+------------------------+----------+-------+-------------+-------------+--------------+\n",
      "| Trial name             | status   | loc   |          lr |   batchSize |   hiddenSize |\n",
      "|------------------------+----------+-------+-------------+-------------+--------------|\n",
      "| train_tune_70cbe_00001 | PENDING  |       | 2.78109e-05 |          16 |           32 |\n",
      "| train_tune_70cbe_00002 | PENDING  |       | 0.000768811 |         128 |           16 |\n",
      "| train_tune_70cbe_00003 | PENDING  |       | 1.34829e-05 |           8 |          128 |\n",
      "| train_tune_70cbe_00004 | PENDING  |       | 0.00024695  |          16 |           16 |\n",
      "| train_tune_70cbe_00000 | ERROR    |       | 4.0697e-05  |         128 |           64 |\n",
      "+------------------------+----------+-------+-------------+-------------+--------------+\n",
      "Number of errored trials: 1\n",
      "+------------------------+--------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "| Trial name             |   # failures | error file                                                                                                                                                    |\n",
      "|------------------------+--------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
      "| train_tune_70cbe_00000 |            1 | D:\\02_Studium\\SBX\\mad-recime\\network\\LSTM\\ray\\version_ReciMe\\train_tune_70cbe_00000_0_batchSize=128,hiddenSize=64,lr=4.0697e-05_2022-04-01_22-51-02\\error.txt |\n",
      "+------------------------+--------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=)\u001b[0m [2022-04-01 22:52:02,498 E 21372 2644] (raylet.exe) worker_pool.cc:481: Some workers of the worker process(14056) have not registered within the timeout. The process is still alive, probably it's hanging during start.\n",
      "\u001b[2m\u001b[36m(pid=)\u001b[0m [2022-04-01 22:52:03,630 E 21372 2644] (raylet.exe) worker_pool.cc:481: Some workers of the worker process(8520) have not registered within the timeout. The process is still alive, probably it's hanging during start.\n",
      "\u001b[2m\u001b[36m(pid=)\u001b[0m [2022-04-01 22:52:03,637 E 21372 2644] (raylet.exe) worker_pool.cc:481: Some workers of the worker process(5652) have not registered within the timeout. The process is still alive, probably it's hanging during start.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Current time: 2022-04-01 22:52:05 (running for 00:01:03.47)\n",
      "Memory usage on this node: 7.2/7.9 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.\n",
      "Using AsyncHyperBand: num_stopped=0\n",
      "Bracket: Iter 16.000: None | Iter 8.000: None | Iter 4.000: None | Iter 2.000: None | Iter 1.000: None\n",
      "Resources requested: 0/4 CPUs, 0/1 GPUs, 0.0/0.6 GiB heap, 0.0/0.3 GiB objects\n",
      "Result logdir: D:\\02_Studium\\SBX\\mad-recime\\network\\LSTM\\ray\\version_ReciMe\n",
      "Number of trials: 5/5 (1 ERROR, 4 PENDING)\n",
      "+------------------------+----------+-------+-------------+-------------+--------------+\n",
      "| Trial name             | status   | loc   |          lr |   batchSize |   hiddenSize |\n",
      "|------------------------+----------+-------+-------------+-------------+--------------|\n",
      "| train_tune_70cbe_00001 | PENDING  |       | 2.78109e-05 |          16 |           32 |\n",
      "| train_tune_70cbe_00002 | PENDING  |       | 0.000768811 |         128 |           16 |\n",
      "| train_tune_70cbe_00003 | PENDING  |       | 1.34829e-05 |           8 |          128 |\n",
      "| train_tune_70cbe_00004 | PENDING  |       | 0.00024695  |          16 |           16 |\n",
      "| train_tune_70cbe_00000 | ERROR    |       | 4.0697e-05  |         128 |           64 |\n",
      "+------------------------+----------+-------+-------------+-------------+--------------+\n",
      "Number of errored trials: 1\n",
      "+------------------------+--------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "| Trial name             |   # failures | error file                                                                                                                                                    |\n",
      "|------------------------+--------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
      "| train_tune_70cbe_00000 |            1 | D:\\02_Studium\\SBX\\mad-recime\\network\\LSTM\\ray\\version_ReciMe\\train_tune_70cbe_00000_0_batchSize=128,hiddenSize=64,lr=4.0697e-05_2022-04-01_22-51-02\\error.txt |\n",
      "+------------------------+--------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-04-01 22:52:10 (running for 00:01:08.48)\n",
      "Memory usage on this node: 7.2/7.9 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.\n",
      "Using AsyncHyperBand: num_stopped=0\n",
      "Bracket: Iter 16.000: None | Iter 8.000: None | Iter 4.000: None | Iter 2.000: None | Iter 1.000: None\n",
      "Resources requested: 0/4 CPUs, 0/1 GPUs, 0.0/0.6 GiB heap, 0.0/0.3 GiB objects\n",
      "Result logdir: D:\\02_Studium\\SBX\\mad-recime\\network\\LSTM\\ray\\version_ReciMe\n",
      "Number of trials: 5/5 (1 ERROR, 4 PENDING)\n",
      "+------------------------+----------+-------+-------------+-------------+--------------+\n",
      "| Trial name             | status   | loc   |          lr |   batchSize |   hiddenSize |\n",
      "|------------------------+----------+-------+-------------+-------------+--------------|\n",
      "| train_tune_70cbe_00001 | PENDING  |       | 2.78109e-05 |          16 |           32 |\n",
      "| train_tune_70cbe_00002 | PENDING  |       | 0.000768811 |         128 |           16 |\n",
      "| train_tune_70cbe_00003 | PENDING  |       | 1.34829e-05 |           8 |          128 |\n",
      "| train_tune_70cbe_00004 | PENDING  |       | 0.00024695  |          16 |           16 |\n",
      "| train_tune_70cbe_00000 | ERROR    |       | 4.0697e-05  |         128 |           64 |\n",
      "+------------------------+----------+-------+-------------+-------------+--------------+\n",
      "Number of errored trials: 1\n",
      "+------------------------+--------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "| Trial name             |   # failures | error file                                                                                                                                                    |\n",
      "|------------------------+--------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
      "| train_tune_70cbe_00000 |            1 | D:\\02_Studium\\SBX\\mad-recime\\network\\LSTM\\ray\\version_ReciMe\\train_tune_70cbe_00000_0_batchSize=128,hiddenSize=64,lr=4.0697e-05_2022-04-01_22-51-02\\error.txt |\n",
      "+------------------------+--------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-01 22:52:22,294\tWARNING worker.py:1326 -- Traceback (most recent call last):\n",
      "  File \"python\\ray\\_raylet.pyx\", line 633, in ray._raylet.execute_task\n",
      "  File \"python\\ray\\_raylet.pyx\", line 674, in ray._raylet.execute_task\n",
      "  File \"python\\ray\\_raylet.pyx\", line 640, in ray._raylet.execute_task\n",
      "  File \"python\\ray\\_raylet.pyx\", line 644, in ray._raylet.execute_task\n",
      "  File \"python\\ray\\_raylet.pyx\", line 593, in ray._raylet.execute_task.function_executor\n",
      "  File \"C:\\Users\\Hannes\\miniconda3\\envs\\madEnv\\lib\\site-packages\\ray\\_private\\function_manager.py\", line 648, in actor_method_executor\n",
      "    return method(__ray_actor, *args, **kwargs)\n",
      "  File \"C:\\Users\\Hannes\\miniconda3\\envs\\madEnv\\lib\\site-packages\\ray\\_private\\function_manager.py\", line 544, in temporary_actor_method\n",
      "    raise RuntimeError(\n",
      "RuntimeError: The actor with name ImplicitFunc failed to import on the worker. This may be because needed library dependencies are not installed in the worker environment:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Hannes\\miniconda3\\envs\\madEnv\\lib\\site-packages\\ray\\_private\\function_manager.py\", line 603, in _load_actor_class_from_gcs\n",
      "    actor_class = pickle.loads(pickled_class)\n",
      "  File \"C:\\Users\\Hannes\\miniconda3\\envs\\madEnv\\lib\\site-packages\\ray\\cloudpickle\\cloudpickle.py\", line 679, in subimport\n",
      "    __import__(name)\n",
      "ModuleNotFoundError: No module named 'src'\n",
      "\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"python\\ray\\_raylet.pyx\", line 774, in ray._raylet.task_execution_handler\n",
      "  File \"python\\ray\\_raylet.pyx\", line 595, in ray._raylet.execute_task\n",
      "  File \"python\\ray\\_raylet.pyx\", line 715, in ray._raylet.execute_task\n",
      "  File \"C:\\Users\\Hannes\\miniconda3\\envs\\madEnv\\lib\\site-packages\\ray\\_private\\function_manager.py\", line 544, in temporary_actor_method\n",
      "    raise RuntimeError(\n",
      "RuntimeError: The actor with name ImplicitFunc failed to import on the worker. This may be because needed library dependencies are not installed in the worker environment:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Hannes\\miniconda3\\envs\\madEnv\\lib\\site-packages\\ray\\_private\\function_manager.py\", line 603, in _load_actor_class_from_gcs\n",
      "    actor_class = pickle.loads(pickled_class)\n",
      "  File \"C:\\Users\\Hannes\\miniconda3\\envs\\madEnv\\lib\\site-packages\\ray\\cloudpickle\\cloudpickle.py\", line 679, in subimport\n",
      "    __import__(name)\n",
      "ModuleNotFoundError: No module named 'src'\n",
      "\n",
      "An unexpected internal error occurred while the worker was executing a task.\n",
      " pid=16256)\u001b[0m 2022-04-01 22:52:22,271\tERROR worker.py:430 -- SystemExit was raised from the worker.\n",
      " pid=16256)\u001b[0m Traceback (most recent call last):\n",
      " pid=16256)\u001b[0m   File \"python\\ray\\_raylet.pyx\", line 633, in ray._raylet.execute_task\n",
      " pid=16256)\u001b[0m   File \"python\\ray\\_raylet.pyx\", line 674, in ray._raylet.execute_task\n",
      " pid=16256)\u001b[0m   File \"python\\ray\\_raylet.pyx\", line 640, in ray._raylet.execute_task\n",
      " pid=16256)\u001b[0m   File \"python\\ray\\_raylet.pyx\", line 644, in ray._raylet.execute_task\n",
      " pid=16256)\u001b[0m   File \"python\\ray\\_raylet.pyx\", line 593, in ray._raylet.execute_task.function_executor\n",
      " pid=16256)\u001b[0m   File \"C:\\Users\\Hannes\\miniconda3\\envs\\madEnv\\lib\\site-packages\\ray\\_private\\function_manager.py\", line 648, in actor_method_executor\n",
      " pid=16256)\u001b[0m     return method(__ray_actor, *args, **kwargs)\n",
      " pid=16256)\u001b[0m   File \"C:\\Users\\Hannes\\miniconda3\\envs\\madEnv\\lib\\site-packages\\ray\\_private\\function_manager.py\", line 544, in temporary_actor_method\n",
      " pid=16256)\u001b[0m     raise RuntimeError(\n",
      " pid=16256)\u001b[0m RuntimeError: The actor with name ImplicitFunc failed to import on the worker. This may be because needed library dependencies are not installed in the worker environment:\n",
      " pid=16256)\u001b[0m aryActor\n",
      " pid=16256)\u001b[0m Traceback (most recent call last):\n",
      " pid=16256)\u001b[0m   File \"C:\\Users\\Hannes\\miniconda3\\envs\\madEnv\\lib\\site-packages\\ray\\_private\\function_manager.py\", line 603, in _load_actor_class_from_gcs\n",
      " pid=16256)\u001b[0m     actor_class = pickle.loads(pickled_class)\n",
      " pid=16256)\u001b[0m   File \"C:\\Users\\Hannes\\miniconda3\\envs\\madEnv\\lib\\site-packages\\ray\\cloudpickle\\cloudpickle.py\", line 679, in subimport\n",
      " pid=16256)\u001b[0m     __import__(name)\n",
      " pid=16256)\u001b[0m ModuleNotFoundError: No module named 'src'\n",
      " pid=16256)\u001b[0m aryActor\n",
      " pid=16256)\u001b[0m aryActor\n",
      " pid=16256)\u001b[0m During handling of the above exception, another exception occurred:\n",
      " pid=16256)\u001b[0m aryActor\n",
      " pid=16256)\u001b[0m Traceback (most recent call last):\n",
      " pid=16256)\u001b[0m   File \"python\\ray\\_raylet.pyx\", line 774, in ray._raylet.task_execution_handler\n",
      " pid=16256)\u001b[0m   File \"python\\ray\\_raylet.pyx\", line 595, in ray._raylet.execute_task\n",
      " pid=16256)\u001b[0m   File \"python\\ray\\_raylet.pyx\", line 715, in ray._raylet.execute_task\n",
      " pid=16256)\u001b[0m   File \"C:\\Users\\Hannes\\miniconda3\\envs\\madEnv\\lib\\site-packages\\ray\\_private\\function_manager.py\", line 544, in temporary_actor_method\n",
      " pid=16256)\u001b[0m     raise RuntimeError(\n",
      " pid=16256)\u001b[0m RuntimeError: The actor with name ImplicitFunc failed to import on the worker. This may be because needed library dependencies are not installed in the worker environment:\n",
      " pid=16256)\u001b[0m aryActor\n",
      " pid=16256)\u001b[0m Traceback (most recent call last):\n",
      " pid=16256)\u001b[0m   File \"C:\\Users\\Hannes\\miniconda3\\envs\\madEnv\\lib\\site-packages\\ray\\_private\\function_manager.py\", line 603, in _load_actor_class_from_gcs\n",
      " pid=16256)\u001b[0m     actor_class = pickle.loads(pickled_class)\n",
      " pid=16256)\u001b[0m   File \"C:\\Users\\Hannes\\miniconda3\\envs\\madEnv\\lib\\site-packages\\ray\\cloudpickle\\cloudpickle.py\", line 679, in subimport\n",
      " pid=16256)\u001b[0m     __import__(name)\n",
      " pid=16256)\u001b[0m ModuleNotFoundError: No module named 'src'\n",
      " pid=16256)\u001b[0m aryActor\n",
      " pid=16256)\u001b[0m aryActor\n",
      " pid=16256)\u001b[0m During handling of the above exception, another exception occurred:\n",
      " pid=16256)\u001b[0m aryActor\n",
      " pid=16256)\u001b[0m Traceback (most recent call last):\n",
      " pid=16256)\u001b[0m   File \"python\\ray\\_raylet.pyx\", line 803, in ray._raylet.task_execution_handler\n",
      " pid=16256)\u001b[0m SystemExit\n",
      "2022-04-01 22:52:22,571\tWARNING worker.py:1326 -- A worker died or was killed while executing a task by an unexpected system error. To troubleshoot the problem, check the logs for the dead worker. RayTask ID: ffffffffffffffffa62c0cfd1d38d939e76affed01000000 Worker ID: 77abe2f467c7fd8c98afb4e7298d8143b2c1709055858d8442233c1a Node ID: fb972aa1896eeec4945941e547fa10f3a552dffd6291b1f357977c0b Worker IP address: 127.0.0.1 Worker port: 65115 Worker PID: 16256\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Current time: 2022-04-01 22:52:16 (running for 00:01:14.39)\n",
      "Memory usage on this node: 7.3/7.9 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.\n",
      "Using AsyncHyperBand: num_stopped=0\n",
      "Bracket: Iter 16.000: None | Iter 8.000: None | Iter 4.000: None | Iter 2.000: None | Iter 1.000: None\n",
      "Resources requested: 1.0/4 CPUs, 0/1 GPUs, 0.0/0.6 GiB heap, 0.0/0.3 GiB objects\n",
      "Result logdir: D:\\02_Studium\\SBX\\mad-recime\\network\\LSTM\\ray\\version_ReciMe\n",
      "Number of trials: 5/5 (1 ERROR, 3 PENDING, 1 RUNNING)\n",
      "+------------------------+----------+-------+-------------+-------------+--------------+\n",
      "| Trial name             | status   | loc   |          lr |   batchSize |   hiddenSize |\n",
      "|------------------------+----------+-------+-------------+-------------+--------------|\n",
      "| train_tune_70cbe_00001 | RUNNING  |       | 2.78109e-05 |          16 |           32 |\n",
      "| train_tune_70cbe_00002 | PENDING  |       | 0.000768811 |         128 |           16 |\n",
      "| train_tune_70cbe_00003 | PENDING  |       | 1.34829e-05 |           8 |          128 |\n",
      "| train_tune_70cbe_00004 | PENDING  |       | 0.00024695  |          16 |           16 |\n",
      "| train_tune_70cbe_00000 | ERROR    |       | 4.0697e-05  |         128 |           64 |\n",
      "+------------------------+----------+-------+-------------+-------------+--------------+\n",
      "Number of errored trials: 1\n",
      "+------------------------+--------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "| Trial name             |   # failures | error file                                                                                                                                                    |\n",
      "|------------------------+--------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
      "| train_tune_70cbe_00000 |            1 | D:\\02_Studium\\SBX\\mad-recime\\network\\LSTM\\ray\\version_ReciMe\\train_tune_70cbe_00000_0_batchSize=128,hiddenSize=64,lr=4.0697e-05_2022-04-01_22-51-02\\error.txt |\n",
      "+------------------------+--------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-01 22:52:32,214\tWARNING worker.py:1326 -- Traceback (most recent call last):\n",
      "  File \"python\\ray\\_raylet.pyx\", line 633, in ray._raylet.execute_task\n",
      "  File \"python\\ray\\_raylet.pyx\", line 674, in ray._raylet.execute_task\n",
      "  File \"python\\ray\\_raylet.pyx\", line 640, in ray._raylet.execute_task\n",
      "  File \"python\\ray\\_raylet.pyx\", line 644, in ray._raylet.execute_task\n",
      "  File \"python\\ray\\_raylet.pyx\", line 593, in ray._raylet.execute_task.function_executor\n",
      "  File \"C:\\Users\\Hannes\\miniconda3\\envs\\madEnv\\lib\\site-packages\\ray\\_private\\function_manager.py\", line 648, in actor_method_executor\n",
      "    return method(__ray_actor, *args, **kwargs)\n",
      "  File \"C:\\Users\\Hannes\\miniconda3\\envs\\madEnv\\lib\\site-packages\\ray\\_private\\function_manager.py\", line 544, in temporary_actor_method\n",
      "    raise RuntimeError(\n",
      "RuntimeError: The actor with name ImplicitFunc failed to import on the worker. This may be because needed library dependencies are not installed in the worker environment:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Hannes\\miniconda3\\envs\\madEnv\\lib\\site-packages\\ray\\_private\\function_manager.py\", line 603, in _load_actor_class_from_gcs\n",
      "    actor_class = pickle.loads(pickled_class)\n",
      "  File \"C:\\Users\\Hannes\\miniconda3\\envs\\madEnv\\lib\\site-packages\\ray\\cloudpickle\\cloudpickle.py\", line 679, in subimport\n",
      "    __import__(name)\n",
      "ModuleNotFoundError: No module named 'src'\n",
      "\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"python\\ray\\_raylet.pyx\", line 774, in ray._raylet.task_execution_handler\n",
      "  File \"python\\ray\\_raylet.pyx\", line 595, in ray._raylet.execute_task\n",
      "  File \"python\\ray\\_raylet.pyx\", line 715, in ray._raylet.execute_task\n",
      "  File \"C:\\Users\\Hannes\\miniconda3\\envs\\madEnv\\lib\\site-packages\\ray\\_private\\function_manager.py\", line 544, in temporary_actor_method\n",
      "    raise RuntimeError(\n",
      "RuntimeError: The actor with name ImplicitFunc failed to import on the worker. This may be because needed library dependencies are not installed in the worker environment:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Hannes\\miniconda3\\envs\\madEnv\\lib\\site-packages\\ray\\_private\\function_manager.py\", line 603, in _load_actor_class_from_gcs\n",
      "    actor_class = pickle.loads(pickled_class)\n",
      "  File \"C:\\Users\\Hannes\\miniconda3\\envs\\madEnv\\lib\\site-packages\\ray\\cloudpickle\\cloudpickle.py\", line 679, in subimport\n",
      "    __import__(name)\n",
      "ModuleNotFoundError: No module named 'src'\n",
      "\n",
      "An unexpected internal error occurred while the worker was executing a task.\n",
      " pid=8588)\u001b[0m 2022-04-01 22:52:32,186\tERROR worker.py:430 -- SystemExit was raised from the worker.\n",
      " pid=8588)\u001b[0m Traceback (most recent call last):\n",
      " pid=8588)\u001b[0m   File \"python\\ray\\_raylet.pyx\", line 633, in ray._raylet.execute_task\n",
      " pid=8588)\u001b[0m   File \"python\\ray\\_raylet.pyx\", line 674, in ray._raylet.execute_task\n",
      " pid=8588)\u001b[0m   File \"python\\ray\\_raylet.pyx\", line 640, in ray._raylet.execute_task\n",
      " pid=8588)\u001b[0m   File \"python\\ray\\_raylet.pyx\", line 644, in ray._raylet.execute_task\n",
      " pid=8588)\u001b[0m   File \"python\\ray\\_raylet.pyx\", line 593, in ray._raylet.execute_task.function_executor\n",
      " pid=8588)\u001b[0m   File \"C:\\Users\\Hannes\\miniconda3\\envs\\madEnv\\lib\\site-packages\\ray\\_private\\function_manager.py\", line 648, in actor_method_executor\n",
      " pid=8588)\u001b[0m     return method(__ray_actor, *args, **kwargs)\n",
      " pid=8588)\u001b[0m   File \"C:\\Users\\Hannes\\miniconda3\\envs\\madEnv\\lib\\site-packages\\ray\\_private\\function_manager.py\", line 544, in temporary_actor_method\n",
      " pid=8588)\u001b[0m     raise RuntimeError(\n",
      " pid=8588)\u001b[0m RuntimeError: The actor with name ImplicitFunc failed to import on the worker. This may be because needed library dependencies are not installed in the worker environment:\n",
      " pid=8588)\u001b[0m raryActor\n",
      " pid=8588)\u001b[0m Traceback (most recent call last):\n",
      " pid=8588)\u001b[0m   File \"C:\\Users\\Hannes\\miniconda3\\envs\\madEnv\\lib\\site-packages\\ray\\_private\\function_manager.py\", line 603, in _load_actor_class_from_gcs\n",
      " pid=8588)\u001b[0m     actor_class = pickle.loads(pickled_class)\n",
      " pid=8588)\u001b[0m   File \"C:\\Users\\Hannes\\miniconda3\\envs\\madEnv\\lib\\site-packages\\ray\\cloudpickle\\cloudpickle.py\", line 679, in subimport\n",
      " pid=8588)\u001b[0m     __import__(name)\n",
      " pid=8588)\u001b[0m ModuleNotFoundError: No module named 'src'\n",
      " pid=8588)\u001b[0m raryActor\n",
      " pid=8588)\u001b[0m raryActor\n",
      " pid=8588)\u001b[0m During handling of the above exception, another exception occurred:\n",
      " pid=8588)\u001b[0m raryActor\n",
      " pid=8588)\u001b[0m Traceback (most recent call last):\n",
      " pid=8588)\u001b[0m   File \"python\\ray\\_raylet.pyx\", line 774, in ray._raylet.task_execution_handler\n",
      " pid=8588)\u001b[0m   File \"python\\ray\\_raylet.pyx\", line 595, in ray._raylet.execute_task\n",
      " pid=8588)\u001b[0m   File \"python\\ray\\_raylet.pyx\", line 715, in ray._raylet.execute_task\n",
      " pid=8588)\u001b[0m   File \"C:\\Users\\Hannes\\miniconda3\\envs\\madEnv\\lib\\site-packages\\ray\\_private\\function_manager.py\", line 544, in temporary_actor_method\n",
      " pid=8588)\u001b[0m     raise RuntimeError(\n",
      " pid=8588)\u001b[0m RuntimeError: The actor with name ImplicitFunc failed to import on the worker. This may be because needed library dependencies are not installed in the worker environment:\n",
      " pid=8588)\u001b[0m raryActor\n",
      " pid=8588)\u001b[0m Traceback (most recent call last):\n",
      " pid=8588)\u001b[0m   File \"C:\\Users\\Hannes\\miniconda3\\envs\\madEnv\\lib\\site-packages\\ray\\_private\\function_manager.py\", line 603, in _load_actor_class_from_gcs\n",
      " pid=8588)\u001b[0m     actor_class = pickle.loads(pickled_class)\n",
      " pid=8588)\u001b[0m   File \"C:\\Users\\Hannes\\miniconda3\\envs\\madEnv\\lib\\site-packages\\ray\\cloudpickle\\cloudpickle.py\", line 679, in subimport\n",
      " pid=8588)\u001b[0m     __import__(name)\n",
      " pid=8588)\u001b[0m ModuleNotFoundError: No module named 'src'\n",
      " pid=8588)\u001b[0m raryActor\n",
      " pid=8588)\u001b[0m raryActor\n",
      " pid=8588)\u001b[0m During handling of the above exception, another exception occurred:\n",
      " pid=8588)\u001b[0m raryActor\n",
      " pid=8588)\u001b[0m Traceback (most recent call last):\n",
      " pid=8588)\u001b[0m   File \"python\\ray\\_raylet.pyx\", line 803, in ray._raylet.task_execution_handler\n",
      " pid=8588)\u001b[0m SystemExit\n",
      "2022-04-01 22:52:32,448\tWARNING worker.py:1326 -- A worker died or was killed while executing a task by an unexpected system error. To troubleshoot the problem, check the logs for the dead worker. RayTask ID: ffffffffffffffff4b4c7b5eb3b71c662006e64501000000 Worker ID: 15c1559d3ab9a28276392d88ed7fdf8e8f07bc1a3704a1409d0b0d81 Node ID: fb972aa1896eeec4945941e547fa10f3a552dffd6291b1f357977c0b Worker IP address: 127.0.0.1 Worker port: 65144 Worker PID: 8588\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Current time: 2022-04-01 22:52:23 (running for 00:01:21.20)\n",
      "Memory usage on this node: 7.3/7.9 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.\n",
      "Using AsyncHyperBand: num_stopped=0\n",
      "Bracket: Iter 16.000: None | Iter 8.000: None | Iter 4.000: None | Iter 2.000: None | Iter 1.000: None\n",
      "Resources requested: 2.0/4 CPUs, 0/1 GPUs, 0.0/0.6 GiB heap, 0.0/0.3 GiB objects\n",
      "Result logdir: D:\\02_Studium\\SBX\\mad-recime\\network\\LSTM\\ray\\version_ReciMe\n",
      "Number of trials: 5/5 (1 ERROR, 2 PENDING, 2 RUNNING)\n",
      "+------------------------+----------+-------+-------------+-------------+--------------+\n",
      "| Trial name             | status   | loc   |          lr |   batchSize |   hiddenSize |\n",
      "|------------------------+----------+-------+-------------+-------------+--------------|\n",
      "| train_tune_70cbe_00001 | RUNNING  |       | 2.78109e-05 |          16 |           32 |\n",
      "| train_tune_70cbe_00002 | RUNNING  |       | 0.000768811 |         128 |           16 |\n",
      "| train_tune_70cbe_00003 | PENDING  |       | 1.34829e-05 |           8 |          128 |\n",
      "| train_tune_70cbe_00004 | PENDING  |       | 0.00024695  |          16 |           16 |\n",
      "| train_tune_70cbe_00000 | ERROR    |       | 4.0697e-05  |         128 |           64 |\n",
      "+------------------------+----------+-------+-------------+-------------+--------------+\n",
      "Number of errored trials: 1\n",
      "+------------------------+--------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "| Trial name             |   # failures | error file                                                                                                                                                    |\n",
      "|------------------------+--------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
      "| train_tune_70cbe_00000 |            1 | D:\\02_Studium\\SBX\\mad-recime\\network\\LSTM\\ray\\version_ReciMe\\train_tune_70cbe_00000_0_batchSize=128,hiddenSize=64,lr=4.0697e-05_2022-04-01_22-51-02\\error.txt |\n",
      "+------------------------+--------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-01 22:52:41,703\tWARNING worker.py:1326 -- Traceback (most recent call last):\n",
      "  File \"python\\ray\\_raylet.pyx\", line 633, in ray._raylet.execute_task\n",
      "  File \"python\\ray\\_raylet.pyx\", line 674, in ray._raylet.execute_task\n",
      "  File \"python\\ray\\_raylet.pyx\", line 640, in ray._raylet.execute_task\n",
      "  File \"python\\ray\\_raylet.pyx\", line 644, in ray._raylet.execute_task\n",
      "  File \"python\\ray\\_raylet.pyx\", line 593, in ray._raylet.execute_task.function_executor\n",
      "  File \"C:\\Users\\Hannes\\miniconda3\\envs\\madEnv\\lib\\site-packages\\ray\\_private\\function_manager.py\", line 648, in actor_method_executor\n",
      "    return method(__ray_actor, *args, **kwargs)\n",
      "  File \"C:\\Users\\Hannes\\miniconda3\\envs\\madEnv\\lib\\site-packages\\ray\\_private\\function_manager.py\", line 544, in temporary_actor_method\n",
      "    raise RuntimeError(\n",
      "RuntimeError: The actor with name ImplicitFunc failed to import on the worker. This may be because needed library dependencies are not installed in the worker environment:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Hannes\\miniconda3\\envs\\madEnv\\lib\\site-packages\\ray\\_private\\function_manager.py\", line 603, in _load_actor_class_from_gcs\n",
      "    actor_class = pickle.loads(pickled_class)\n",
      "  File \"C:\\Users\\Hannes\\miniconda3\\envs\\madEnv\\lib\\site-packages\\ray\\cloudpickle\\cloudpickle.py\", line 679, in subimport\n",
      "    __import__(name)\n",
      "ModuleNotFoundError: No module named 'src'\n",
      "\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"python\\ray\\_raylet.pyx\", line 774, in ray._raylet.task_execution_handler\n",
      "  File \"python\\ray\\_raylet.pyx\", line 595, in ray._raylet.execute_task\n",
      "  File \"python\\ray\\_raylet.pyx\", line 715, in ray._raylet.execute_task\n",
      "  File \"C:\\Users\\Hannes\\miniconda3\\envs\\madEnv\\lib\\site-packages\\ray\\_private\\function_manager.py\", line 544, in temporary_actor_method\n",
      "    raise RuntimeError(\n",
      "RuntimeError: The actor with name ImplicitFunc failed to import on the worker. This may be because needed library dependencies are not installed in the worker environment:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Hannes\\miniconda3\\envs\\madEnv\\lib\\site-packages\\ray\\_private\\function_manager.py\", line 603, in _load_actor_class_from_gcs\n",
      "    actor_class = pickle.loads(pickled_class)\n",
      "  File \"C:\\Users\\Hannes\\miniconda3\\envs\\madEnv\\lib\\site-packages\\ray\\cloudpickle\\cloudpickle.py\", line 679, in subimport\n",
      "    __import__(name)\n",
      "ModuleNotFoundError: No module named 'src'\n",
      "\n",
      "An unexpected internal error occurred while the worker was executing a task.\n",
      " pid=11808)\u001b[0m 2022-04-01 22:52:41,656\tERROR worker.py:430 -- SystemExit was raised from the worker.\n",
      " pid=11808)\u001b[0m Traceback (most recent call last):\n",
      " pid=11808)\u001b[0m   File \"python\\ray\\_raylet.pyx\", line 633, in ray._raylet.execute_task\n",
      " pid=11808)\u001b[0m   File \"python\\ray\\_raylet.pyx\", line 674, in ray._raylet.execute_task\n",
      " pid=11808)\u001b[0m   File \"python\\ray\\_raylet.pyx\", line 640, in ray._raylet.execute_task\n",
      " pid=11808)\u001b[0m   File \"python\\ray\\_raylet.pyx\", line 644, in ray._raylet.execute_task\n",
      " pid=11808)\u001b[0m   File \"python\\ray\\_raylet.pyx\", line 593, in ray._raylet.execute_task.function_executor\n",
      " pid=11808)\u001b[0m   File \"C:\\Users\\Hannes\\miniconda3\\envs\\madEnv\\lib\\site-packages\\ray\\_private\\function_manager.py\", line 648, in actor_method_executor\n",
      " pid=11808)\u001b[0m     return method(__ray_actor, *args, **kwargs)\n",
      " pid=11808)\u001b[0m   File \"C:\\Users\\Hannes\\miniconda3\\envs\\madEnv\\lib\\site-packages\\ray\\_private\\function_manager.py\", line 544, in temporary_actor_method\n",
      " pid=11808)\u001b[0m     raise RuntimeError(\n",
      " pid=11808)\u001b[0m RuntimeError: The actor with name ImplicitFunc failed to import on the worker. This may be because needed library dependencies are not installed in the worker environment:\n",
      " pid=11808)\u001b[0m aryActor\n",
      " pid=11808)\u001b[0m Traceback (most recent call last):\n",
      " pid=11808)\u001b[0m   File \"C:\\Users\\Hannes\\miniconda3\\envs\\madEnv\\lib\\site-packages\\ray\\_private\\function_manager.py\", line 603, in _load_actor_class_from_gcs\n",
      " pid=11808)\u001b[0m     actor_class = pickle.loads(pickled_class)\n",
      " pid=11808)\u001b[0m   File \"C:\\Users\\Hannes\\miniconda3\\envs\\madEnv\\lib\\site-packages\\ray\\cloudpickle\\cloudpickle.py\", line 679, in subimport\n",
      " pid=11808)\u001b[0m     __import__(name)\n",
      " pid=11808)\u001b[0m ModuleNotFoundError: No module named 'src'\n",
      " pid=11808)\u001b[0m aryActor\n",
      " pid=11808)\u001b[0m aryActor\n",
      " pid=11808)\u001b[0m During handling of the above exception, another exception occurred:\n",
      " pid=11808)\u001b[0m aryActor\n",
      " pid=11808)\u001b[0m Traceback (most recent call last):\n",
      " pid=11808)\u001b[0m   File \"python\\ray\\_raylet.pyx\", line 774, in ray._raylet.task_execution_handler\n",
      " pid=11808)\u001b[0m   File \"python\\ray\\_raylet.pyx\", line 595, in ray._raylet.execute_task\n",
      " pid=11808)\u001b[0m   File \"python\\ray\\_raylet.pyx\", line 715, in ray._raylet.execute_task\n",
      " pid=11808)\u001b[0m   File \"C:\\Users\\Hannes\\miniconda3\\envs\\madEnv\\lib\\site-packages\\ray\\_private\\function_manager.py\", line 544, in temporary_actor_method\n",
      " pid=11808)\u001b[0m     raise RuntimeError(\n",
      " pid=11808)\u001b[0m RuntimeError: The actor with name ImplicitFunc failed to import on the worker. This may be because needed library dependencies are not installed in the worker environment:\n",
      "2022-04-01 22:52:41,833\tWARNING worker.py:1326 -- A worker died or was killed while executing a task by an unexpected system error. To troubleshoot the problem, check the logs for the dead worker. RayTask ID: ffffffffffffffffd341b1d9e692a111b0a3622701000000 Worker ID: cd9b92602d26c1e0c36e8431c3e39a3a2304cfb0249986ef88b397f8 Node ID: fb972aa1896eeec4945941e547fa10f3a552dffd6291b1f357977c0b Worker IP address: 127.0.0.1 Worker port: 65138 Worker PID: 11808\n",
      "\n",
      " pid=11808)\u001b[0m Traceback (most recent call last):\n",
      " pid=11808)\u001b[0m   File \"C:\\Users\\Hannes\\miniconda3\\envs\\madEnv\\lib\\site-packages\\ray\\_private\\function_manager.py\", line 603, in _load_actor_class_from_gcs\n",
      " pid=11808)\u001b[0m     actor_class = pickle.loads(pickled_class)\n",
      " pid=11808)\u001b[0m   File \"C:\\Users\\Hannes\\miniconda3\\envs\\madEnv\\lib\\site-packages\\ray\\cloudpickle\\cloudpickle.py\", line 679, in subimport\n",
      " pid=11808)\u001b[0m     __import__(name)\n",
      " pid=11808)\u001b[0m ModuleNotFoundError: No module named 'src'\n",
      " pid=11808)\u001b[0m aryActor\n",
      " pid=11808)\u001b[0m aryActor\n",
      " pid=11808)\u001b[0m During handling of the above exception, another exception occurred:\n",
      " pid=11808)\u001b[0m aryActor\n",
      " pid=11808)\u001b[0m Traceback (most recent call last):\n",
      " pid=11808)\u001b[0m   File \"python\\ray\\_raylet.pyx\", line 803, in ray._raylet.task_execution_handler\n",
      " pid=11808)\u001b[0m SystemExit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Current time: 2022-04-01 22:52:32 (running for 00:01:30.80)\n",
      "Memory usage on this node: 7.3/7.9 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.\n",
      "Using AsyncHyperBand: num_stopped=0\n",
      "Bracket: Iter 16.000: None | Iter 8.000: None | Iter 4.000: None | Iter 2.000: None | Iter 1.000: None\n",
      "Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/0.6 GiB heap, 0.0/0.3 GiB objects\n",
      "Result logdir: D:\\02_Studium\\SBX\\mad-recime\\network\\LSTM\\ray\\version_ReciMe\n",
      "Number of trials: 5/5 (1 ERROR, 1 PENDING, 3 RUNNING)\n",
      "+------------------------+----------+-------+-------------+-------------+--------------+\n",
      "| Trial name             | status   | loc   |          lr |   batchSize |   hiddenSize |\n",
      "|------------------------+----------+-------+-------------+-------------+--------------|\n",
      "| train_tune_70cbe_00001 | RUNNING  |       | 2.78109e-05 |          16 |           32 |\n",
      "| train_tune_70cbe_00002 | RUNNING  |       | 0.000768811 |         128 |           16 |\n",
      "| train_tune_70cbe_00003 | RUNNING  |       | 1.34829e-05 |           8 |          128 |\n",
      "| train_tune_70cbe_00004 | PENDING  |       | 0.00024695  |          16 |           16 |\n",
      "| train_tune_70cbe_00000 | ERROR    |       | 4.0697e-05  |         128 |           64 |\n",
      "+------------------------+----------+-------+-------------+-------------+--------------+\n",
      "Number of errored trials: 1\n",
      "+------------------------+--------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "| Trial name             |   # failures | error file                                                                                                                                                    |\n",
      "|------------------------+--------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
      "| train_tune_70cbe_00000 |            1 | D:\\02_Studium\\SBX\\mad-recime\\network\\LSTM\\ray\\version_ReciMe\\train_tune_70cbe_00000_0_batchSize=128,hiddenSize=64,lr=4.0697e-05_2022-04-01_22-51-02\\error.txt |\n",
      "+------------------------+--------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-01 22:52:50,314\tWARNING worker.py:1326 -- Traceback (most recent call last):\n",
      "  File \"python\\ray\\_raylet.pyx\", line 633, in ray._raylet.execute_task\n",
      "  File \"python\\ray\\_raylet.pyx\", line 674, in ray._raylet.execute_task\n",
      "  File \"python\\ray\\_raylet.pyx\", line 640, in ray._raylet.execute_task\n",
      "  File \"python\\ray\\_raylet.pyx\", line 644, in ray._raylet.execute_task\n",
      "  File \"python\\ray\\_raylet.pyx\", line 593, in ray._raylet.execute_task.function_executor\n",
      "  File \"C:\\Users\\Hannes\\miniconda3\\envs\\madEnv\\lib\\site-packages\\ray\\_private\\function_manager.py\", line 648, in actor_method_executor\n",
      "    return method(__ray_actor, *args, **kwargs)\n",
      "  File \"C:\\Users\\Hannes\\miniconda3\\envs\\madEnv\\lib\\site-packages\\ray\\_private\\function_manager.py\", line 544, in temporary_actor_method\n",
      "    raise RuntimeError(\n",
      "RuntimeError: The actor with name ImplicitFunc failed to import on the worker. This may be because needed library dependencies are not installed in the worker environment:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Hannes\\miniconda3\\envs\\madEnv\\lib\\site-packages\\ray\\_private\\function_manager.py\", line 603, in _load_actor_class_from_gcs\n",
      "    actor_class = pickle.loads(pickled_class)\n",
      "  File \"C:\\Users\\Hannes\\miniconda3\\envs\\madEnv\\lib\\site-packages\\ray\\cloudpickle\\cloudpickle.py\", line 679, in subimport\n",
      "    __import__(name)\n",
      "ModuleNotFoundError: No module named 'src'\n",
      "\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"python\\ray\\_raylet.pyx\", line 774, in ray._raylet.task_execution_handler\n",
      "  File \"python\\ray\\_raylet.pyx\", line 595, in ray._raylet.execute_task\n",
      "  File \"python\\ray\\_raylet.pyx\", line 715, in ray._raylet.execute_task\n",
      "  File \"C:\\Users\\Hannes\\miniconda3\\envs\\madEnv\\lib\\site-packages\\ray\\_private\\function_manager.py\", line 544, in temporary_actor_method\n",
      "    raise RuntimeError(\n",
      "RuntimeError: The actor with name ImplicitFunc failed to import on the worker. This may be because needed library dependencies are not installed in the worker environment:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Hannes\\miniconda3\\envs\\madEnv\\lib\\site-packages\\ray\\_private\\function_manager.py\", line 603, in _load_actor_class_from_gcs\n",
      "    actor_class = pickle.loads(pickled_class)\n",
      "  File \"C:\\Users\\Hannes\\miniconda3\\envs\\madEnv\\lib\\site-packages\\ray\\cloudpickle\\cloudpickle.py\", line 679, in subimport\n",
      "    __import__(name)\n",
      "ModuleNotFoundError: No module named 'src'\n",
      "\n",
      "An unexpected internal error occurred while the worker was executing a task.\n",
      "2022-04-01 22:52:50,466\tWARNING worker.py:1326 -- A worker died or was killed while executing a task by an unexpected system error. To troubleshoot the problem, check the logs for the dead worker. RayTask ID: ffffffffffffffff3d02b1080a9e97e642cfbc4401000000 Worker ID: a39b33c2f23c5b9da57678fcd17ec45638429f1bf9f95bcc6df20402 Node ID: fb972aa1896eeec4945941e547fa10f3a552dffd6291b1f357977c0b Worker IP address: 127.0.0.1 Worker port: 65130 Worker PID: 23608\n",
      " pid=23608)\u001b[0m 2022-04-01 22:52:50,308\tERROR worker.py:430 -- SystemExit was raised from the worker.\n",
      " pid=23608)\u001b[0m Traceback (most recent call last):\n",
      " pid=23608)\u001b[0m   File \"python\\ray\\_raylet.pyx\", line 633, in ray._raylet.execute_task\n",
      " pid=23608)\u001b[0m   File \"python\\ray\\_raylet.pyx\", line 674, in ray._raylet.execute_task\n",
      " pid=23608)\u001b[0m   File \"python\\ray\\_raylet.pyx\", line 640, in ray._raylet.execute_task\n",
      " pid=23608)\u001b[0m   File \"python\\ray\\_raylet.pyx\", line 644, in ray._raylet.execute_task\n",
      " pid=23608)\u001b[0m   File \"python\\ray\\_raylet.pyx\", line 593, in ray._raylet.execute_task.function_executor\n",
      " pid=23608)\u001b[0m   File \"C:\\Users\\Hannes\\miniconda3\\envs\\madEnv\\lib\\site-packages\\ray\\_private\\function_manager.py\", line 648, in actor_method_executor\n",
      " pid=23608)\u001b[0m     return method(__ray_actor, *args, **kwargs)\n",
      " pid=23608)\u001b[0m   File \"C:\\Users\\Hannes\\miniconda3\\envs\\madEnv\\lib\\site-packages\\ray\\_private\\function_manager.py\", line 544, in temporary_actor_method\n",
      " pid=23608)\u001b[0m     raise RuntimeError(\n",
      " pid=23608)\u001b[0m RuntimeError: The actor with name ImplicitFunc failed to import on the worker. This may be because needed library dependencies are not installed in the worker environment:\n",
      " pid=23608)\u001b[0m aryActor\n",
      " pid=23608)\u001b[0m Traceback (most recent call last):\n",
      " pid=23608)\u001b[0m   File \"C:\\Users\\Hannes\\miniconda3\\envs\\madEnv\\lib\\site-packages\\ray\\_private\\function_manager.py\", line 603, in _load_actor_class_from_gcs\n",
      " pid=23608)\u001b[0m     actor_class = pickle.loads(pickled_class)\n",
      " pid=23608)\u001b[0m   File \"C:\\Users\\Hannes\\miniconda3\\envs\\madEnv\\lib\\site-packages\\ray\\cloudpickle\\cloudpickle.py\", line 679, in subimport\n",
      " pid=23608)\u001b[0m     __import__(name)\n",
      " pid=23608)\u001b[0m ModuleNotFoundError: No module named 'src'\n",
      " pid=23608)\u001b[0m aryActor\n",
      " pid=23608)\u001b[0m aryActor\n",
      " pid=23608)\u001b[0m During handling of the above exception, another exception occurred:\n",
      " pid=23608)\u001b[0m aryActor\n",
      " pid=23608)\u001b[0m Traceback (most recent call last):\n",
      " pid=23608)\u001b[0m   File \"python\\ray\\_raylet.pyx\", line 774, in ray._raylet.task_execution_handler\n",
      " pid=23608)\u001b[0m   File \"python\\ray\\_raylet.pyx\", line 595, in ray._raylet.execute_task\n",
      " pid=23608)\u001b[0m   File \"python\\ray\\_raylet.pyx\", line 715, in ray._raylet.execute_task\n",
      " pid=23608)\u001b[0m   File \"C:\\Users\\Hannes\\miniconda3\\envs\\madEnv\\lib\\site-packages\\ray\\_private\\function_manager.py\", line 544, in temporary_actor_method\n",
      " pid=23608)\u001b[0m     raise RuntimeError(\n",
      " pid=23608)\u001b[0m RuntimeError: The actor with name ImplicitFunc failed to import on the worker. This may be because needed library dependencies are not installed in the worker environment:\n",
      " pid=23608)\u001b[0m aryActor\n",
      " pid=23608)\u001b[0m Traceback (most recent call last):\n",
      " pid=23608)\u001b[0m   File \"C:\\Users\\Hannes\\miniconda3\\envs\\madEnv\\lib\\site-packages\\ray\\_private\\function_manager.py\", line 603, in _load_actor_class_from_gcs\n",
      " pid=23608)\u001b[0m     actor_class = pickle.loads(pickled_class)\n",
      " pid=23608)\u001b[0m   File \"C:\\Users\\Hannes\\miniconda3\\envs\\madEnv\\lib\\site-packages\\ray\\cloudpickle\\cloudpickle.py\", line 679, in subimport\n",
      " pid=23608)\u001b[0m     __import__(name)\n",
      " pid=23608)\u001b[0m ModuleNotFoundError: No module named 'src'\n",
      " pid=23608)\u001b[0m aryActor\n",
      " pid=23608)\u001b[0m aryActor\n",
      " pid=23608)\u001b[0m During handling of the above exception, another exception occurred:\n",
      " pid=23608)\u001b[0m aryActor\n",
      " pid=23608)\u001b[0m Traceback (most recent call last):\n",
      " pid=23608)\u001b[0m   File \"python\\ray\\_raylet.pyx\", line 803, in ray._raylet.task_execution_handler\n",
      " pid=23608)\u001b[0m SystemExit\n",
      "2022-04-01 22:52:50,557\tWARNING ray_trial_executor.py:655 -- Over the last 60 seconds, the Tune event loop has been backlogged processing new results. Consider increasing your period of result reporting to improve performance.\n",
      "2022-04-01 22:52:50,561\tERROR trial_runner.py:920 -- Trial train_tune_70cbe_00002: Error processing event.\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Hannes\\miniconda3\\envs\\madEnv\\lib\\site-packages\\ray\\tune\\trial_runner.py\", line 886, in _process_trial\n",
      "    results = self.trial_executor.fetch_result(trial)\n",
      "  File \"C:\\Users\\Hannes\\miniconda3\\envs\\madEnv\\lib\\site-packages\\ray\\tune\\ray_trial_executor.py\", line 675, in fetch_result\n",
      "    result = ray.get(trial_future[0], timeout=DEFAULT_GET_TIMEOUT)\n",
      "  File \"C:\\Users\\Hannes\\miniconda3\\envs\\madEnv\\lib\\site-packages\\ray\\_private\\client_mode_hook.py\", line 105, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"C:\\Users\\Hannes\\miniconda3\\envs\\madEnv\\lib\\site-packages\\ray\\worker.py\", line 1765, in get\n",
      "    raise value\n",
      "ray.exceptions.RayActorError: The actor died unexpectedly before finishing this task.\n",
      "\tclass_name: ImplicitFunc\n",
      "\tactor_id: 4b4c7b5eb3b71c662006e64501000000\n",
      "\tpid: 8588\n",
      "\tnamespace: dfa27098-ba22-48a7-a645-ed4d5ff7d961\n",
      "\tip: 127.0.0.1\n",
      "The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR_EXIT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Current time: 2022-04-01 22:52:42 (running for 00:01:40.29)\n",
      "Memory usage on this node: 7.3/7.9 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.\n",
      "Using AsyncHyperBand: num_stopped=0\n",
      "Bracket: Iter 16.000: None | Iter 8.000: None | Iter 4.000: None | Iter 2.000: None | Iter 1.000: None\n",
      "Resources requested: 4.0/4 CPUs, 0/1 GPUs, 0.0/0.6 GiB heap, 0.0/0.3 GiB objects\n",
      "Result logdir: D:\\02_Studium\\SBX\\mad-recime\\network\\LSTM\\ray\\version_ReciMe\n",
      "Number of trials: 5/5 (1 ERROR, 4 RUNNING)\n",
      "+------------------------+----------+-------+-------------+-------------+--------------+\n",
      "| Trial name             | status   | loc   |          lr |   batchSize |   hiddenSize |\n",
      "|------------------------+----------+-------+-------------+-------------+--------------|\n",
      "| train_tune_70cbe_00001 | RUNNING  |       | 2.78109e-05 |          16 |           32 |\n",
      "| train_tune_70cbe_00002 | RUNNING  |       | 0.000768811 |         128 |           16 |\n",
      "| train_tune_70cbe_00003 | RUNNING  |       | 1.34829e-05 |           8 |          128 |\n",
      "| train_tune_70cbe_00004 | RUNNING  |       | 0.00024695  |          16 |           16 |\n",
      "| train_tune_70cbe_00000 | ERROR    |       | 4.0697e-05  |         128 |           64 |\n",
      "+------------------------+----------+-------+-------------+-------------+--------------+\n",
      "Number of errored trials: 1\n",
      "+------------------------+--------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "| Trial name             |   # failures | error file                                                                                                                                                    |\n",
      "|------------------------+--------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
      "| train_tune_70cbe_00000 |            1 | D:\\02_Studium\\SBX\\mad-recime\\network\\LSTM\\ray\\version_ReciMe\\train_tune_70cbe_00000_0_batchSize=128,hiddenSize=64,lr=4.0697e-05_2022-04-01_22-51-02\\error.txt |\n",
      "+------------------------+--------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n",
      "Result for train_tune_70cbe_00002:\n",
      "  trial_id: 70cbe_00002\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-01 22:52:50,796\tERROR trial_runner.py:920 -- Trial train_tune_70cbe_00003: Error processing event.\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Hannes\\miniconda3\\envs\\madEnv\\lib\\site-packages\\ray\\tune\\trial_runner.py\", line 886, in _process_trial\n",
      "    results = self.trial_executor.fetch_result(trial)\n",
      "  File \"C:\\Users\\Hannes\\miniconda3\\envs\\madEnv\\lib\\site-packages\\ray\\tune\\ray_trial_executor.py\", line 675, in fetch_result\n",
      "    result = ray.get(trial_future[0], timeout=DEFAULT_GET_TIMEOUT)\n",
      "  File \"C:\\Users\\Hannes\\miniconda3\\envs\\madEnv\\lib\\site-packages\\ray\\_private\\client_mode_hook.py\", line 105, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"C:\\Users\\Hannes\\miniconda3\\envs\\madEnv\\lib\\site-packages\\ray\\worker.py\", line 1765, in get\n",
      "    raise value\n",
      "ray.exceptions.RayActorError: The actor died unexpectedly before finishing this task.\n",
      "\tclass_name: ImplicitFunc\n",
      "\tactor_id: d341b1d9e692a111b0a3622701000000\n",
      "\tpid: 11808\n",
      "\tnamespace: dfa27098-ba22-48a7-a645-ed4d5ff7d961\n",
      "\tip: 127.0.0.1\n",
      "The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR_EXIT\n",
      "2022-04-01 22:52:50,939\tERROR trial_runner.py:920 -- Trial train_tune_70cbe_00004: Error processing event.\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Hannes\\miniconda3\\envs\\madEnv\\lib\\site-packages\\ray\\tune\\trial_runner.py\", line 886, in _process_trial\n",
      "    results = self.trial_executor.fetch_result(trial)\n",
      "  File \"C:\\Users\\Hannes\\miniconda3\\envs\\madEnv\\lib\\site-packages\\ray\\tune\\ray_trial_executor.py\", line 675, in fetch_result\n",
      "    result = ray.get(trial_future[0], timeout=DEFAULT_GET_TIMEOUT)\n",
      "  File \"C:\\Users\\Hannes\\miniconda3\\envs\\madEnv\\lib\\site-packages\\ray\\_private\\client_mode_hook.py\", line 105, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"C:\\Users\\Hannes\\miniconda3\\envs\\madEnv\\lib\\site-packages\\ray\\worker.py\", line 1765, in get\n",
      "    raise value\n",
      "ray.exceptions.RayActorError: The actor died unexpectedly before finishing this task.\n",
      "\tclass_name: ImplicitFunc\n",
      "\tactor_id: 3d02b1080a9e97e642cfbc4401000000\n",
      "\tpid: 23608\n",
      "\tnamespace: dfa27098-ba22-48a7-a645-ed4d5ff7d961\n",
      "\tip: 127.0.0.1\n",
      "The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR_EXIT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Current time: 2022-04-01 22:52:50 (running for 00:01:48.86)\n",
      "Memory usage on this node: 7.3/7.9 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.\n",
      "Using AsyncHyperBand: num_stopped=0\n",
      "Bracket: Iter 16.000: None | Iter 8.000: None | Iter 4.000: None | Iter 2.000: None | Iter 1.000: None\n",
      "Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/0.6 GiB heap, 0.0/0.3 GiB objects\n",
      "Result logdir: D:\\02_Studium\\SBX\\mad-recime\\network\\LSTM\\ray\\version_ReciMe\n",
      "Number of trials: 5/5 (2 ERROR, 3 RUNNING)\n",
      "+------------------------+----------+-------+-------------+-------------+--------------+\n",
      "| Trial name             | status   | loc   |          lr |   batchSize |   hiddenSize |\n",
      "|------------------------+----------+-------+-------------+-------------+--------------|\n",
      "| train_tune_70cbe_00001 | RUNNING  |       | 2.78109e-05 |          16 |           32 |\n",
      "| train_tune_70cbe_00003 | RUNNING  |       | 1.34829e-05 |           8 |          128 |\n",
      "| train_tune_70cbe_00004 | RUNNING  |       | 0.00024695  |          16 |           16 |\n",
      "| train_tune_70cbe_00000 | ERROR    |       | 4.0697e-05  |         128 |           64 |\n",
      "| train_tune_70cbe_00002 | ERROR    |       | 0.000768811 |         128 |           16 |\n",
      "+------------------------+----------+-------+-------------+-------------+--------------+\n",
      "Number of errored trials: 2\n",
      "+------------------------+--------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "| Trial name             |   # failures | error file                                                                                                                                                    |\n",
      "|------------------------+--------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
      "| train_tune_70cbe_00000 |            1 | D:\\02_Studium\\SBX\\mad-recime\\network\\LSTM\\ray\\version_ReciMe\\train_tune_70cbe_00000_0_batchSize=128,hiddenSize=64,lr=4.0697e-05_2022-04-01_22-51-02\\error.txt |\n",
      "| train_tune_70cbe_00002 |            1 | D:\\02_Studium\\SBX\\mad-recime\\network\\LSTM\\ray\\version_ReciMe\\train_tune_70cbe_00002_2_batchSize=128,hiddenSize=16,lr=0.00076881_2022-04-01_22-52-12\\error.txt |\n",
      "+------------------------+--------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n",
      "Result for train_tune_70cbe_00003:\n",
      "  trial_id: 70cbe_00003\n",
      "  \n",
      "Result for train_tune_70cbe_00004:\n",
      "  trial_id: 70cbe_00004\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-01 22:52:51,117\tERROR trial_runner.py:920 -- Trial train_tune_70cbe_00001: Error processing event.\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Hannes\\miniconda3\\envs\\madEnv\\lib\\site-packages\\ray\\tune\\trial_runner.py\", line 886, in _process_trial\n",
      "    results = self.trial_executor.fetch_result(trial)\n",
      "  File \"C:\\Users\\Hannes\\miniconda3\\envs\\madEnv\\lib\\site-packages\\ray\\tune\\ray_trial_executor.py\", line 675, in fetch_result\n",
      "    result = ray.get(trial_future[0], timeout=DEFAULT_GET_TIMEOUT)\n",
      "  File \"C:\\Users\\Hannes\\miniconda3\\envs\\madEnv\\lib\\site-packages\\ray\\_private\\client_mode_hook.py\", line 105, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"C:\\Users\\Hannes\\miniconda3\\envs\\madEnv\\lib\\site-packages\\ray\\worker.py\", line 1765, in get\n",
      "    raise value\n",
      "ray.exceptions.RayActorError: The actor died unexpectedly before finishing this task.\n",
      "\tclass_name: ImplicitFunc\n",
      "\tactor_id: a62c0cfd1d38d939e76affed01000000\n",
      "\tpid: 16256\n",
      "\tnamespace: dfa27098-ba22-48a7-a645-ed4d5ff7d961\n",
      "\tip: 127.0.0.1\n",
      "The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR_EXIT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for train_tune_70cbe_00001:\n",
      "  trial_id: 70cbe_00001\n",
      "  \n",
      "== Status ==\n",
      "Current time: 2022-04-01 22:52:51 (running for 00:01:49.30)\n",
      "Memory usage on this node: 7.3/7.9 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.\n",
      "Using AsyncHyperBand: num_stopped=0\n",
      "Bracket: Iter 16.000: None | Iter 8.000: None | Iter 4.000: None | Iter 2.000: None | Iter 1.000: None\n",
      "Resources requested: 0/4 CPUs, 0/1 GPUs, 0.0/0.6 GiB heap, 0.0/0.3 GiB objects\n",
      "Result logdir: D:\\02_Studium\\SBX\\mad-recime\\network\\LSTM\\ray\\version_ReciMe\n",
      "Number of trials: 5/5 (5 ERROR)\n",
      "+------------------------+----------+-------+-------------+-------------+--------------+\n",
      "| Trial name             | status   | loc   |          lr |   batchSize |   hiddenSize |\n",
      "|------------------------+----------+-------+-------------+-------------+--------------|\n",
      "| train_tune_70cbe_00000 | ERROR    |       | 4.0697e-05  |         128 |           64 |\n",
      "| train_tune_70cbe_00001 | ERROR    |       | 2.78109e-05 |          16 |           32 |\n",
      "| train_tune_70cbe_00002 | ERROR    |       | 0.000768811 |         128 |           16 |\n",
      "| train_tune_70cbe_00003 | ERROR    |       | 1.34829e-05 |           8 |          128 |\n",
      "| train_tune_70cbe_00004 | ERROR    |       | 0.00024695  |          16 |           16 |\n",
      "+------------------------+----------+-------+-------------+-------------+--------------+\n",
      "Number of errored trials: 5\n",
      "+------------------------+--------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "| Trial name             |   # failures | error file                                                                                                                                                    |\n",
      "|------------------------+--------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
      "| train_tune_70cbe_00000 |            1 | D:\\02_Studium\\SBX\\mad-recime\\network\\LSTM\\ray\\version_ReciMe\\train_tune_70cbe_00000_0_batchSize=128,hiddenSize=64,lr=4.0697e-05_2022-04-01_22-51-02\\error.txt |\n",
      "| train_tune_70cbe_00001 |            1 | D:\\02_Studium\\SBX\\mad-recime\\network\\LSTM\\ray\\version_ReciMe\\train_tune_70cbe_00001_1_batchSize=16,hiddenSize=32,lr=2.7811e-05_2022-04-01_22-51-17\\error.txt  |\n",
      "| train_tune_70cbe_00002 |            1 | D:\\02_Studium\\SBX\\mad-recime\\network\\LSTM\\ray\\version_ReciMe\\train_tune_70cbe_00002_2_batchSize=128,hiddenSize=16,lr=0.00076881_2022-04-01_22-52-12\\error.txt |\n",
      "| train_tune_70cbe_00003 |            1 | D:\\02_Studium\\SBX\\mad-recime\\network\\LSTM\\ray\\version_ReciMe\\train_tune_70cbe_00003_3_batchSize=8,hiddenSize=128,lr=1.3483e-05_2022-04-01_22-52-32\\error.txt  |\n",
      "| train_tune_70cbe_00004 |            1 | D:\\02_Studium\\SBX\\mad-recime\\network\\LSTM\\ray\\version_ReciMe\\train_tune_70cbe_00004_4_batchSize=16,hiddenSize=16,lr=0.00024695_2022-04-01_22-52-42\\error.txt  |\n",
      "+------------------------+--------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    },
    {
     "ename": "TuneError",
     "evalue": "('Trials did not complete', [train_tune_70cbe_00000, train_tune_70cbe_00001, train_tune_70cbe_00002, train_tune_70cbe_00003, train_tune_70cbe_00004])",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTuneError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32md:\\02_Studium\\SBX\\mad-recime\\network\\LSTM\\src\\nlp_utils\\trainTitle.ipynb Cell 15'\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/02_Studium/SBX/mad-recime/network/LSTM/src/nlp_utils/trainTitle.ipynb#ch0000014?line=0'>1</a>\u001b[0m \u001b[39m# start hyperparameter optimization\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/02_Studium/SBX/mad-recime/network/LSTM/src/nlp_utils/trainTitle.ipynb#ch0000014?line=1'>2</a>\u001b[0m analysis \u001b[39m=\u001b[39m tune\u001b[39m.\u001b[39;49mrun(\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/02_Studium/SBX/mad-recime/network/LSTM/src/nlp_utils/trainTitle.ipynb#ch0000014?line=2'>3</a>\u001b[0m     tune\u001b[39m.\u001b[39;49mwith_parameters(\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/02_Studium/SBX/mad-recime/network/LSTM/src/nlp_utils/trainTitle.ipynb#ch0000014?line=3'>4</a>\u001b[0m         train_tune, callbacks\u001b[39m=\u001b[39;49mcallbacks, epochs\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m\u001b[39m#, gpus=0\u001b[39;49;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/02_Studium/SBX/mad-recime/network/LSTM/src/nlp_utils/trainTitle.ipynb#ch0000014?line=4'>5</a>\u001b[0m     ),\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/02_Studium/SBX/mad-recime/network/LSTM/src/nlp_utils/trainTitle.ipynb#ch0000014?line=5'>6</a>\u001b[0m     config\u001b[39m=\u001b[39;49mconfig,\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/02_Studium/SBX/mad-recime/network/LSTM/src/nlp_utils/trainTitle.ipynb#ch0000014?line=6'>7</a>\u001b[0m     num_samples\u001b[39m=\u001b[39;49mconfig[\u001b[39m'\u001b[39;49m\u001b[39mnum_trials\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/02_Studium/SBX/mad-recime/network/LSTM/src/nlp_utils/trainTitle.ipynb#ch0000014?line=7'>8</a>\u001b[0m     local_dir\u001b[39m=\u001b[39;49mrayDir,\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/02_Studium/SBX/mad-recime/network/LSTM/src/nlp_utils/trainTitle.ipynb#ch0000014?line=8'>9</a>\u001b[0m     name\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mversion_ReciMe\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/02_Studium/SBX/mad-recime/network/LSTM/src/nlp_utils/trainTitle.ipynb#ch0000014?line=9'>10</a>\u001b[0m     metric\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mloss\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/02_Studium/SBX/mad-recime/network/LSTM/src/nlp_utils/trainTitle.ipynb#ch0000014?line=10'>11</a>\u001b[0m     mode\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mmin\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/02_Studium/SBX/mad-recime/network/LSTM/src/nlp_utils/trainTitle.ipynb#ch0000014?line=11'>12</a>\u001b[0m     scheduler\u001b[39m=\u001b[39;49mscheduler,\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/02_Studium/SBX/mad-recime/network/LSTM/src/nlp_utils/trainTitle.ipynb#ch0000014?line=12'>13</a>\u001b[0m     progress_reporter\u001b[39m=\u001b[39;49mreporter,\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/02_Studium/SBX/mad-recime/network/LSTM/src/nlp_utils/trainTitle.ipynb#ch0000014?line=13'>14</a>\u001b[0m )\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/02_Studium/SBX/mad-recime/network/LSTM/src/nlp_utils/trainTitle.ipynb#ch0000014?line=15'>16</a>\u001b[0m \u001b[39m# get some information from the optimization\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/02_Studium/SBX/mad-recime/network/LSTM/src/nlp_utils/trainTitle.ipynb#ch0000014?line=16'>17</a>\u001b[0m best_trial \u001b[39m=\u001b[39m analysis\u001b[39m.\u001b[39mbest_trial  \u001b[39m# Get best trial\u001b[39;00m\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\madEnv\\lib\\site-packages\\ray\\tune\\tune.py:633\u001b[0m, in \u001b[0;36mrun\u001b[1;34m(run_or_experiment, name, metric, mode, stop, time_budget_s, config, resources_per_trial, num_samples, local_dir, search_alg, scheduler, keep_checkpoints_num, checkpoint_score_attr, checkpoint_freq, checkpoint_at_end, verbose, progress_reporter, log_to_file, trial_name_creator, trial_dirname_creator, sync_config, export_formats, max_failures, fail_fast, restore, server_port, resume, reuse_actors, trial_executor, raise_on_failed_trial, callbacks, max_concurrent_trials, queue_trials, loggers, _remote)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/Hannes/miniconda3/envs/madEnv/lib/site-packages/ray/tune/tune.py?line=630'>631</a>\u001b[0m \u001b[39mif\u001b[39;00m incomplete_trials:\n\u001b[0;32m    <a href='file:///c%3A/Users/Hannes/miniconda3/envs/madEnv/lib/site-packages/ray/tune/tune.py?line=631'>632</a>\u001b[0m     \u001b[39mif\u001b[39;00m raise_on_failed_trial \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m state[signal\u001b[39m.\u001b[39mSIGINT]:\n\u001b[1;32m--> <a href='file:///c%3A/Users/Hannes/miniconda3/envs/madEnv/lib/site-packages/ray/tune/tune.py?line=632'>633</a>\u001b[0m         \u001b[39mraise\u001b[39;00m TuneError(\u001b[39m\"\u001b[39m\u001b[39mTrials did not complete\u001b[39m\u001b[39m\"\u001b[39m, incomplete_trials)\n\u001b[0;32m    <a href='file:///c%3A/Users/Hannes/miniconda3/envs/madEnv/lib/site-packages/ray/tune/tune.py?line=633'>634</a>\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    <a href='file:///c%3A/Users/Hannes/miniconda3/envs/madEnv/lib/site-packages/ray/tune/tune.py?line=634'>635</a>\u001b[0m         logger\u001b[39m.\u001b[39merror(\u001b[39m\"\u001b[39m\u001b[39mTrials did not complete: \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m, incomplete_trials)\n",
      "\u001b[1;31mTuneError\u001b[0m: ('Trials did not complete', [train_tune_70cbe_00000, train_tune_70cbe_00001, train_tune_70cbe_00002, train_tune_70cbe_00003, train_tune_70cbe_00004])"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-02 11:12:43,035\tWARNING worker.py:1326 -- The node with node id: fb972aa1896eeec4945941e547fa10f3a552dffd6291b1f357977c0b and ip: 127.0.0.1 has been marked dead because the detector has missed too many heartbeats from it. This can happen when a raylet crashes unexpectedly or has lagging heartbeats.\n"
     ]
    }
   ],
   "source": [
    "# start hyperparameter optimization\n",
    "analysis = tune.run(\n",
    "    tune.with_parameters(\n",
    "        train_tune, callbacks=callbacks, epochs=1#, gpus=0\n",
    "    ),\n",
    "    config=config,\n",
    "    num_samples=config['num_trials'],\n",
    "    local_dir=rayDir,\n",
    "    name='version_ReciMe',\n",
    "    metric='loss',\n",
    "    mode='min',\n",
    "    scheduler=scheduler,\n",
    "    progress_reporter=reporter,\n",
    ")\n",
    "\n",
    "# get some information from the optimization\n",
    "best_trial = analysis.best_trial  # Get best trial\n",
    "best_config = analysis.best_config  # Get best trial's hyperparameters\n",
    "best_logdir = analysis.best_logdir  # Get best trial's logdir\n",
    "best_checkpoint = analysis.best_checkpoint  # Get best trial's best checkpoint\n",
    "best_result = analysis.best_result  # Get best trial's last results\n",
    "best_result_df = analysis.best_result_df  # Get best result as pandas dataframe\n",
    "\n",
    "# Get a dataframe with the last results for each trial\n",
    "df_results = analysis.results_df\n",
    "\n",
    "# Get a dataframe of results for a specific score or mode\n",
    "df = analysis.dataframe(metric=\"loss\", mode=\"min\")\n",
    "# df2 = analysis.dataframe(metric=\"val_epoch_F1\", mode=\"max\") # check how to include multiple metrics\n",
    "\n",
    "\n",
    "print(\"Best hyperparameters found were: \", analysis.best_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a78eccb40e61b98140ae1027d27413ec0b4019861790215488952e2bb9987dd2"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('madEnv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
