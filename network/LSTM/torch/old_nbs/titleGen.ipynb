{"cells":[{"cell_type":"markdown","metadata":{"id":"vzCPQy6dlIJK"},"source":["https://pytorch.org/tutorials/beginner/nlp/sequence_models_tutorial.html"]},{"cell_type":"markdown","metadata":{"id":"8R3yLJ1_xBA2"},"source":["## Basic includes"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3444,"status":"ok","timestamp":1648370824856,"user":{"displayName":"Hannes Schatz","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gid5OGy83mtec_o2eRuSouQCJdeaTjs7eanU6bo=s64","userId":"17509193172932182254"},"user_tz":-120},"id":"1XN0Eb2xPKgV","outputId":"26912a43-b419-43ad-bf78-8abd10f4aba6"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting ipython-autotime\n","  Downloading ipython_autotime-0.3.1-py2.py3-none-any.whl (6.8 kB)\n","Requirement already satisfied: ipython in /usr/local/lib/python3.7/dist-packages (from ipython-autotime) (5.5.0)\n","Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.7/dist-packages (from ipython->ipython-autotime) (1.0.18)\n","Requirement already satisfied: pexpect in /usr/local/lib/python3.7/dist-packages (from ipython->ipython-autotime) (4.8.0)\n","Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython->ipython-autotime) (4.4.2)\n","Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.7/dist-packages (from ipython->ipython-autotime) (0.8.1)\n","Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython->ipython-autotime) (2.6.1)\n","Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.7/dist-packages (from ipython->ipython-autotime) (57.4.0)\n","Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython->ipython-autotime) (0.7.5)\n","Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.7/dist-packages (from ipython->ipython-autotime) (5.1.1)\n","Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython->ipython-autotime) (1.15.0)\n","Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython->ipython-autotime) (0.2.5)\n","Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.7/dist-packages (from pexpect->ipython->ipython-autotime) (0.7.0)\n","Installing collected packages: ipython-autotime\n","Successfully installed ipython-autotime-0.3.1\n","time: 1.71 ms (started: 2022-03-27 08:47:03 +00:00)\n"]}],"source":["!pip install ipython-autotime\n","%load_ext autotime"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":24776,"status":"ok","timestamp":1648370849615,"user":{"displayName":"Hannes Schatz","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gid5OGy83mtec_o2eRuSouQCJdeaTjs7eanU6bo=s64","userId":"17509193172932182254"},"user_tz":-120},"id":"jE0vTXeQQv_A","outputId":"d5972aba-72ea-4a1b-f94e-231af96cfb96"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting word2vec\n","  Downloading word2vec-0.11.1.tar.gz (42 kB)\n","\u001b[?25l\r\u001b[K     |███████▊                        | 10 kB 44.6 MB/s eta 0:00:01\r\u001b[K     |███████████████▌                | 20 kB 42.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▎        | 30 kB 22.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 40 kB 14.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 42 kB 1.1 MB/s \n","\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: numpy>=1.9.2 in /usr/local/lib/python3.7/dist-packages (from word2vec) (1.21.5)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from word2vec) (1.1.0)\n","Building wheels for collected packages: word2vec\n","  Building wheel for word2vec (PEP 517) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for word2vec: filename=word2vec-0.11.1-py2.py3-none-any.whl size=164791 sha256=1c79fbd8f48ea1888d04397dcf772e5edc7cde5366bebfd99db4c2fe150e4e95\n","  Stored in directory: /root/.cache/pip/wheels/c9/c0/d4/29d797817e268124a32b6cf8beb8b8fe87b86f099d5a049e61\n","Successfully built word2vec\n","Installing collected packages: word2vec\n","Successfully installed word2vec-0.11.1\n","Collecting ray[tune]\n","  Downloading ray-1.11.0-cp37-cp37m-manylinux2014_x86_64.whl (52.7 MB)\n","\u001b[K     |████████████████████████████████| 52.7 MB 1.2 MB/s \n","\u001b[?25hRequirement already satisfied: numpy>=1.16 in /usr/local/lib/python3.7/dist-packages (from ray[tune]) (1.21.5)\n","Requirement already satisfied: protobuf>=3.15.3 in /usr/local/lib/python3.7/dist-packages (from ray[tune]) (3.17.3)\n","Collecting grpcio<=1.43.0,>=1.28.1\n","  Downloading grpcio-1.43.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.1 MB)\n","\u001b[K     |████████████████████████████████| 4.1 MB 64.2 MB/s \n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from ray[tune]) (3.6.0)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from ray[tune]) (3.13)\n","Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.7/dist-packages (from ray[tune]) (7.1.2)\n","Requirement already satisfied: msgpack<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from ray[tune]) (1.0.3)\n","Collecting redis>=3.5.0\n","  Downloading redis-4.2.0-py3-none-any.whl (225 kB)\n","\u001b[K     |████████████████████████████████| 225 kB 97.7 MB/s \n","\u001b[?25hRequirement already satisfied: attrs in /usr/local/lib/python3.7/dist-packages (from ray[tune]) (21.4.0)\n","Requirement already satisfied: jsonschema in /usr/local/lib/python3.7/dist-packages (from ray[tune]) (4.3.3)\n","Collecting tensorboardX>=1.9\n","  Downloading tensorboardX-2.5-py2.py3-none-any.whl (125 kB)\n","\u001b[K     |████████████████████████████████| 125 kB 94.9 MB/s \n","\u001b[?25hRequirement already satisfied: tabulate in /usr/local/lib/python3.7/dist-packages (from ray[tune]) (0.8.9)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from ray[tune]) (2.23.0)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from ray[tune]) (1.3.5)\n","Requirement already satisfied: six>=1.5.2 in /usr/local/lib/python3.7/dist-packages (from grpcio<=1.43.0,>=1.28.1->ray[tune]) (1.15.0)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from redis>=3.5.0->ray[tune]) (3.10.0.2)\n","Collecting deprecated>=1.2.3\n","  Downloading Deprecated-1.2.13-py2.py3-none-any.whl (9.6 kB)\n","Collecting async-timeout>=4.0.2\n","  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n","Requirement already satisfied: importlib-metadata>=1.0 in /usr/local/lib/python3.7/dist-packages (from redis>=3.5.0->ray[tune]) (4.11.3)\n","Requirement already satisfied: packaging>=20.4 in /usr/local/lib/python3.7/dist-packages (from redis>=3.5.0->ray[tune]) (21.3)\n","Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.7/dist-packages (from deprecated>=1.2.3->redis>=3.5.0->ray[tune]) (1.14.0)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=1.0->redis>=3.5.0->ray[tune]) (3.7.0)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.4->redis>=3.5.0->ray[tune]) (3.0.7)\n","Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema->ray[tune]) (0.18.1)\n","Requirement already satisfied: importlib-resources>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema->ray[tune]) (5.4.0)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->ray[tune]) (2.8.2)\n","Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->ray[tune]) (2018.9)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->ray[tune]) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->ray[tune]) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->ray[tune]) (2021.10.8)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->ray[tune]) (3.0.4)\n","Installing collected packages: deprecated, async-timeout, redis, grpcio, tensorboardX, ray\n","  Attempting uninstall: grpcio\n","    Found existing installation: grpcio 1.44.0\n","    Uninstalling grpcio-1.44.0:\n","      Successfully uninstalled grpcio-1.44.0\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","tensorflow 2.8.0 requires tf-estimator-nightly==2.8.0.dev2021122109, which is not installed.\u001b[0m\n","Successfully installed async-timeout-4.0.2 deprecated-1.2.13 grpcio-1.43.0 ray-1.11.0 redis-4.2.0 tensorboardX-2.5\n","time: 25.1 s (started: 2022-03-27 08:47:03 +00:00)\n"]}],"source":["!pip install word2vec\n","!pip install ray[tune]"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":448,"status":"ok","timestamp":1648370850056,"user":{"displayName":"Hannes Schatz","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gid5OGy83mtec_o2eRuSouQCJdeaTjs7eanU6bo=s64","userId":"17509193172932182254"},"user_tz":-120},"id":"e9lWMi0pxADs","outputId":"e349e2b1-1581-43b0-f03a-71b7a85a6450"},"outputs":[{"output_type":"stream","name":"stdout","text":["time: 299 ms (started: 2022-03-27 08:47:28 +00:00)\n"]}],"source":["import word2vec\n","from collections import Counter # https://pymotw.com/2/collections/counter.html\n","\n","import pandas as pd\n","import numpy as np\n","import itertools\n","import re\n","import os\n","\n","import glob"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":37290,"status":"ok","timestamp":1648370887340,"user":{"displayName":"Hannes Schatz","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gid5OGy83mtec_o2eRuSouQCJdeaTjs7eanU6bo=s64","userId":"17509193172932182254"},"user_tz":-120},"id":"06MHgolQxDOM","outputId":"e09d8114-391e-48e7-e79b-78a9ed5ef831"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive/\n","time: 37.4 s (started: 2022-03-27 08:47:28 +00:00)\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive/')\n","\n","dataPath = '/content/drive/MyDrive/TP2/Datasets/Recipe1M/'\n","import sys\n","sys.path.append(dataPath)"]},{"cell_type":"markdown","metadata":{"id":"wEaAmmFMx-FW"},"source":["## Import Data"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":18,"status":"ok","timestamp":1648370887341,"user":{"displayName":"Hannes Schatz","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gid5OGy83mtec_o2eRuSouQCJdeaTjs7eanU6bo=s64","userId":"17509193172932182254"},"user_tz":-120},"id":"9cLUewNUx_uv","outputId":"3f78494c-d450-4563-d1fb-d8c3c552facd"},"outputs":[{"output_type":"stream","name":"stdout","text":["time: 1.05 ms (started: 2022-03-27 08:48:06 +00:00)\n"]}],"source":["TIMESTAMP = '2022_03_19'"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":255},"executionInfo":{"elapsed":266841,"status":"ok","timestamp":1648371154169,"user":{"displayName":"Hannes Schatz","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gid5OGy83mtec_o2eRuSouQCJdeaTjs7eanU6bo=s64","userId":"17509193172932182254"},"user_tz":-120},"id":"UOcytg8qyAvB","outputId":"ac56ba63-220d-4fa2-db34-404eb7dc113f"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["                                         title  \\\n","id                                               \n","000033e39b         Dilly Macaroni Salad Recipe   \n","000035f7ed                            Gazpacho   \n","00003a70b1           Crunchy Onion Potato Bake   \n","00004320bb  Cool 'n Easy Creamy Watermelon Pie   \n","0000631d90          Easy Tropical Beef Skillet   \n","\n","                                                  ingredients  \\\n","id                                                              \n","000033e39b     amount        unit       ingredient\n","0    1....   \n","000035f7ed     amount unit          ingredient\n","0     8.0  ...   \n","00003a70b1     amount   unit             ingredient\n","0    2...   \n","00004320bb     amount   unit            ingredient\n","0    3....   \n","0000631d90     amount        unit             ingredient\n","0...   \n","\n","                                                 instructions  \n","id                                                             \n","000033e39b  0    Cook macaroni according to package direct...  \n","000035f7ed  0    Add the tomatoes to a food processor with...  \n","00003a70b1  0              Preheat oven to 350 degrees Fah...  \n","00004320bb  0     Dissolve Jello in boiling water.\n","1      ...  \n","0000631d90  0    In a large skillet, toast the coconut ove...  "],"text/html":["\n","  <div id=\"df-4cdaaf21-b342-4da9-88a7-265cdbfd5fbe\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>title</th>\n","      <th>ingredients</th>\n","      <th>instructions</th>\n","    </tr>\n","    <tr>\n","      <th>id</th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>000033e39b</th>\n","      <td>Dilly Macaroni Salad Recipe</td>\n","      <td>amount        unit       ingredient\n","0    1....</td>\n","      <td>0    Cook macaroni according to package direct...</td>\n","    </tr>\n","    <tr>\n","      <th>000035f7ed</th>\n","      <td>Gazpacho</td>\n","      <td>amount unit          ingredient\n","0     8.0  ...</td>\n","      <td>0    Add the tomatoes to a food processor with...</td>\n","    </tr>\n","    <tr>\n","      <th>00003a70b1</th>\n","      <td>Crunchy Onion Potato Bake</td>\n","      <td>amount   unit             ingredient\n","0    2...</td>\n","      <td>0              Preheat oven to 350 degrees Fah...</td>\n","    </tr>\n","    <tr>\n","      <th>00004320bb</th>\n","      <td>Cool 'n Easy Creamy Watermelon Pie</td>\n","      <td>amount   unit            ingredient\n","0    3....</td>\n","      <td>0     Dissolve Jello in boiling water.\n","1      ...</td>\n","    </tr>\n","    <tr>\n","      <th>0000631d90</th>\n","      <td>Easy Tropical Beef Skillet</td>\n","      <td>amount        unit             ingredient\n","0...</td>\n","      <td>0    In a large skillet, toast the coconut ove...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-4cdaaf21-b342-4da9-88a7-265cdbfd5fbe')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-4cdaaf21-b342-4da9-88a7-265cdbfd5fbe button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-4cdaaf21-b342-4da9-88a7-265cdbfd5fbe');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":6},{"output_type":"stream","name":"stdout","text":["time: 4min 26s (started: 2022-03-27 08:48:06 +00:00)\n"]}],"source":["baseFrame = pd.DataFrame()\n","\n","smallSet = False\n","\n","if(os.path.exists(dataPath + TIMESTAMP + '/recipes_valid_full.pkl')):\n","  baseFrame = pd.read_pickle(dataPath + TIMESTAMP + '/recipes_valid_full.pkl')\n","elif(smallSet == True):\n","  baseFrame = baseFrame.append(pd.read_pickle(glob.glob(dataPath + TIMESTAMP +  '/recipes_valid_*.pkl')[0]))\n","elif(len(glob.glob(dataPath + TIMESTAMP +  '/recipes_valid_*.pkl')) != 0):\n","  for file in glob.glob(dataPath + TIMESTAMP +  '/recipes_valid_*.pkl'):\n","    if not 'full' in file:\n","      baseFrame = baseFrame.append(pd.read_pickle(file))\n","\n","baseFrame.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OIncRPSKwy3r"},"outputs":[],"source":["def getAmount(row):\n","  return row['amount'].tolist()\n","def getUnit(row):\n","  return row['unit'].tolist()\n","def getIng(row):\n","  return row['ingredient'].tolist()\n","\n","baseFrame['amount'] = np.vectorize(getAmount, otypes=[np.ndarray])(baseFrame['ingredients'])\n","baseFrame['unit'] = np.vectorize(getUnit, otypes=[np.ndarray])(baseFrame['ingredients'])\n","baseFrame['ingredient'] = np.vectorize(getIng, otypes=[np.ndarray])(baseFrame['ingredients'])\n","baseFrame = baseFrame.drop(columns=['ingredients'])\n","baseFrame.head()"]},{"cell_type":"markdown","metadata":{"id":"lf20_SrBw-i-"},"source":["## Imports for Learning\n","https://pytorch.org/tutorials/beginner/introyt/trainingyt.html"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5pwzWep0k_99"},"outputs":[],"source":["import torch\n","\n","# Model\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.autograd import Variable \n","from torchsummary import summary\n","\n","# Optimizer\n","import torch.optim as optim\n","from torch.utils.data import Dataset, DataLoader, random_split\n","\n","# Tokenizer\n","# torch padding does only support constant padding (ConstantPad1d) for 1D or non-constant padding for >1D (nn.function.pad)\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","# keras tokenizer more powerful than torch\n","from tensorflow.keras.preprocessing.text import Tokenizer, text_to_word_sequence\n","from torchtext.data import get_tokenizer # https://pytorch.org/text/stable/data_utils.html\n","\n","# PyTorch TensorBoard support\n","from torch.utils.tensorboard import SummaryWriter\n","from datetime import datetime\n","\n","# hyperparameter tuning\n","from ray import tune\n","from ray.tune import CLIReporter\n","from ray.tune.schedulers import ASHAScheduler"]},{"cell_type":"markdown","metadata":{"id":"5pOeZxpgBOaY"},"source":["# Seed"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Jjrp4aPbBL-r"},"outputs":[],"source":["torch.manual_seed(0)\n","np.random.seed(0)"]},{"cell_type":"markdown","metadata":{"id":"jIhRxE1cj5ps"},"source":["# Setup\n","https://closeheat.com/blog/pytorch-lstm-text-generation-tutorial\n","\n","\n","## Tokenization\n","to be checked: necessity of punctuation (maybe reintroduce later: https://stackoverflow.com/questions/49073673/include-punctuation-in-keras-tokenizer)"]},{"cell_type":"markdown","metadata":{"id":"m5XkLuiCIbKS"},"source":["### Get Corpus"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jER6BpRl8ikx"},"outputs":[],"source":["w2v_model = word2vec.load(dataPath + 'vocab.bin')\n","ingredientDict = {}\n","for voc in w2v_model.vocab:\n","     # Offset by 1 so empty fields can be 0\n","     ingredientDict.setdefault(voc, len(ingredientDict)+1)\n","\n","if 'dilly' in ingredientDict:\n","  print(\"Word exists\")\n","else:\n","  print('vocab.bin not to be used as dict misses words') "]},{"cell_type":"markdown","metadata":{"id":"yTSZL7Xka-qr"},"source":["### Hyperparams"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SGKlvcMia9Ts"},"outputs":[],"source":["class HyperParams():\n","  def __init__(self, epochs=10, batchSize=10, lr=1e-3, ratio=[0.7, 0.2, 0.1]):\n","    self.epochs = epochs\n","    self.batchSize = batchSize\n","    self.lr = lr\n","    self.ratio = ratio\n","\n","    # self.input_size = 5 #number of features\n","    self.hidden_dim = 4 #number of features in hidden state\n","    self.num_layers = 1 #number of stacked lstm layers\n","    # self.num_classes = 1 #number of output classes \n","    self.embedding_dim = 200 # embedding dimension\n","\n","  def __str__(self):\n","    return('epochs ' + str(self.epochs) + '\\n' +\n","    'batchSize ' + str(self.batchSize) + '\\n' +\n","    'lr ' + str(self.lr) + '\\n' +\n","    'ratio train|val|test ' + str(self.ratio) + '\\n' +\n","    # 'input_size ' + str(self.input_size) + '\\n' +\n","    'hidden_dim ' + str(self.hidden_dim) + '\\n' +\n","    'num_layers ' + str(self.num_layers) + '\\n' +\n","    # 'num_classes ' + str(self.num_classes) + '\\n' +\n","    'embedding_dim ' + str(self.embedding_dim) + '\\n')\n","    "]},{"cell_type":"markdown","metadata":{"id":"WcnDjUaHKVHn"},"source":["### Dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eRKA1xo1thoy"},"outputs":[],"source":["class TitleDataset(Dataset):\n","    def __init__(self, hyperparams, data):\n","      self.hyperparams = hyperparams\n","\n","      self.tokenizer = Tokenizer(oov_token='OOV')\n","\n","      # dataset split into word sequences required for training\n","      self.wordSeq = np.vectorize(self.getTitleSequence, otypes=[np.ndarray])(data['title'], data['ingredient'])\n","\n","      # training requires same length sequences -->  padding\n","      self.maxSequenceLength = max([len(seq['ings']) for seq in self.wordSeq])\n","\n","      # list of all words in dataset\n","      self.words = np.concatenate(np.vectorize(self.getCorpus, otypes=[np.ndarray])(data['title'], data['ingredient']))\n","\n","      # tokenization corpus\n","      self.tokenizer.fit_on_texts(self.words)\n","\n","      # indexed wordSequences (could be calculated in getter but very slow, preprocessing better)\n","      self.idxWords = np.vectorize(self.getIndexedSeqs, otypes=[np.ndarray])(self.wordSeq)\n","\n","      # n gram sequences\n","      self.movWindSeq = pd.Series(np.vectorize(self.getMovWindSeq, otypes=[np.ndarray])(self.idxWords)).explode()\n","      self.movWindSeq.dropna(inplace=True)\n","      self.movWindSeq = self.movWindSeq.to_numpy()\n","\n","\n","    def getCorpus(self, title, ingredient):\n","      titleTok = text_to_word_sequence(title)\n","      ingTok = text_to_word_sequence(','.join(ingredient))\n","      return np.array(ingTok + titleTok)\n","\n","    def getTitleSequence(self, title, ingredient):\n","      titleTok = text_to_word_sequence(title)\n","      ingTok = text_to_word_sequence(','.join(ingredient))\n","      return {'ings': ingTok, 'title': titleTok}\n","\n","    def getIndexedSeqs(self, seq):\n","      ingTok = self.tokenizer.texts_to_sequences([seq['ings']])[0]\n","      ingTok = pad_sequences([ingTok], maxlen=self.maxSequenceLength, padding='pre', value=1)[0] # https://arxiv.org/abs/1903.07288\n","      titleTok = self.tokenizer.texts_to_sequences([seq['title']])[0]\n","\n","      return {'ings': ingTok, 'title': titleTok}\n","\n","    def getMovWindSeq(self, seq):\n","      # input needs to be pre padded\n","      idxShift = len(seq['title'])\n","      ingLen = len(seq['ings'])\n","\n","      fullSeq = np.append(seq['ings'], seq['title'])\n","      retSeq = np.empty((0,ingLen + 1), dtype=np.int32)\n","\n","      for i_shift in range(idxShift):\n","        retSeq = np.vstack([retSeq, np.array(fullSeq[i_shift:ingLen+i_shift+1])])\n","      return retSeq\n","\n","    def __len__(self):\n","        return len(self.idxWords)\n","\n","    def __getitem__(self, index):\n","      # tuple of input (ingredients) and label (title)\n","        return (\n","            torch.tensor(self.movWindSeq[index][:-1]),\n","            torch.tensor(self.movWindSeq[index][1:])\n","        )"]},{"cell_type":"markdown","metadata":{"id":"gukTWG9C5kAR"},"source":["## Model\n","LSTM Net: https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html\n","\n","Embedding Net: https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html\n","\n","Init state: https://stats.stackexchange.com/questions/224737/best-way-to-initialize-lstm-state"]},{"cell_type":"markdown","metadata":{"id":"QtJQI0JktCJe"},"source":["### base: https://github.com/yuchenlin/lstm_sentence_classifier/blob/master/LSTM_sentence_classifier.py\n","\n","### base: https://pytorch.org/tutorials/beginner/nlp/sequence_models_tutorial.html#lstms-in-pytorch\n","\n","### base: https://www.analyticsvidhya.com/blog/2020/08/build-a-natural-language-generation-nlg-system-using-pytorch/"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6XByY-PwpBlI"},"outputs":[],"source":["class EmbedLSTM(nn.Module):\n","\n","    def __init__(self, hyperParams, dataset, device):\n","        super(EmbedLSTM, self).__init__()\n","\n","        # initialize vital params\n","        self.vocab_size = len(dataset.tokenizer.word_index)\n","        self.batchSize = hyperParams.batchSize\n","        self.hidden_dim = hyperParams.hidden_dim\n","        self.device = device\n","        self.num_layers = hyperParams.num_layers\n","        \n","        # embedding definition \n","        self.word_embeddings = nn.Embedding(self.vocab_size, hyperParams.embedding_dim)\n","\n","        # lstm definition\n","        self.lstm = nn.LSTM(input_size=hyperParams.embedding_dim, hidden_size=self.hidden_dim, num_layers=self.num_layers, batch_first=True)\n","\n","        # definition fully connected layer\n","        self.linear = nn.Linear(self.hidden_dim, self.vocab_size)\n","\n","    def forward(self, x, hidden):\n","        embeds = self.word_embeddings(x)\n","\n","        lstm_out, hidden = self.lstm(embeds, hidden)\n","\n","        out = self.linear(lstm_out.reshape(-1, self.hidden_dim))\n","        # tag_scores = F.log_softmax(tag_space, dim=1)\n","        return out, hidden\n","\n","    def init_hidden(self, batchSize=None):\n","        ''' initializes hidden state '''\n","        # Create two new tensors with sizes num_layers x batchSize x hidden_dim,\n","        # initialized to zero, for hidden state and cell state of LSTM\n","        weight = next(self.parameters()).data\n","\n","        batchSize = self.batchSize if batchSize == None else batchSize\n","\n","        hidden = (weight.new(self.num_layers, batchSize, self.hidden_dim).zero_().to(self.device),\n","                  weight.new(self.num_layers, batchSize, self.hidden_dim).zero_().to(self.device))\n","        \n","        return hidden"]},{"cell_type":"markdown","metadata":{"id":"vyuY9T8a5pqr"},"source":["## Training\n","mixture of \n","* https://pytorch.org/tutorials/beginner/introyt/trainingyt.html\n","* https://pytorch.org/tutorials/beginner/hyperparameter_tuning_tutorial.html\n","* https://stackoverflow.com/questions/67295494/correct-validation-loss-in-pytorch\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hbilmeqIMagF"},"outputs":[],"source":["def train_epoch(epoch, model, criterion, optimizer, train_loader, device, writer):\n","  running_loss = 0.\n","  correct = 0\n","  total = 0\n","\n","  h = model.init_hidden()\n","\n","  model.train()\n","\n","  for batch, (input, target) in enumerate(train_loader):\n","    if epoch == 0 and batch == 0:\n","      writer.add_graph(model, input_to_model=(input.to(device), h), verbose=False)\n","\n","    # assign input and target to device\n","    input, target = input.to(device), target.to(device)\n","\n","    # detach hidden states\n","    h = tuple([each.data for each in h])\n","\n","    # clear gradients\n","    optimizer.zero_grad()\n","\n","    # batch prediction (alternative: forward)\n","    outputs, h = model(input, h)\n","    target = target.long()\n","\n","    # loss computation\n","    loss = criterion(outputs, target.view(-1))\n","\n","    # calc backward gradients\n","    loss.backward()\n","\n","    # run optimizer\n","    optimizer.step()\n","\n","    # print statistics\n","    running_loss += loss.item()\n","\n","    # _, predicted = outputs.max(1)\n","    # print(outputs.shape)\n","    # print(predicted.shape)\n","    # total += target.size(0)\n","    # correct += predicted.eq(target).sum().item()\n","\n","  print(\"Epoch: %d, loss: %1.5f\" % (epoch+1, running_loss / len(train_loader)))\n","  return( running_loss / len(train_loader))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nzJxiyTtNdXR"},"outputs":[],"source":["def val_epoch(epoch, model, criterion, optimizer, val_loader, device, writer):\n","  # Validation Loss\n","  correct = 0                                               \n","  total = 0                                                 \n","  running_loss = 0.0    \n","\n","  h = model.init_hidden()                                 \n","      \n","  model.eval() # what does it do\n","  with torch.no_grad(): # what does it do\n","    for batch, (input, target) in enumerate(val_loader):\n","      # assign input and target to device\n","      input, target = input.to(device), target.to(device)\n","\n","      # detach hidden states\n","      h = tuple([each.data for each in h])\n","\n","      # batch prediction (alternative: forward)\n","      outputs, h = model(input, h)\n","      target = target.long()\n","\n","      # loss computation\n","      loss = criterion(outputs, target.view(-1))\n","\n","      # _, predicted = torch.max(outputs.data, 1)\n","      # total += target.size(0)\n","      # correct += (predicted == target).sum().item()\n","\n","      running_loss += loss.item()\n","  # # mean_val_accuracy = (100 * correct / total)               \n","  mean_val_loss = ( running_loss )   \n","  # # print('Validation Accuracy: %d %%' % (mean_val_accuracy)) \n","  # print('Validation Loss:'  ,mean_val_loss )\n","  return( running_loss / len(val_loader))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"U258NxuRDRRx"},"outputs":[],"source":["def train(dataset, model, hyperparams, device):\n","  timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n","  trainWriter = SummaryWriter('/content/drive/MyDrive/runs/titleTrainer/train'.format(timestamp))\n","  valWriter = SummaryWriter('/content/drive/MyDrive/runs/titleTrainer/validation'.format(timestamp))\n","  # writer = SummaryWriter('/content/drive/MyDrive/runs/titleTrainer')\n","\n","  criterion = nn.CrossEntropyLoss()\n","  optimizer = optim.Adam(model.parameters(), lr=hyperparams.lr)\n","\n","  # split data\n","  train_set, val_set = dataset['train'], dataset['val']\n","\n","  train_loader = DataLoader(train_set, batch_size=hyperparams.batchSize, drop_last=True)\n","  val_loader   = DataLoader(val_set, batch_size=hyperparams.batchSize, drop_last=True)\n","  # further options: shuffle, num_workers\n","\n","  for epoch in range(hyperparams.epochs):\n","    trainLoss = train_epoch(epoch, model, criterion, optimizer, train_loader, device, trainWriter)\n","    valLoss = val_epoch(epoch, model, criterion, optimizer, val_loader, device, valWriter)\n","\n","    trainWriter.add_scalar('loss', trainLoss, epoch)  \n","    valWriter.add_scalar('loss', valLoss, epoch)  \n","    # writer.add_scalars('loss', {'train':trainLoss,\n","    #                                 'val':valLoss}, epoch)\n","\n","  trainWriter.flush()\n","  valWriter.flush()\n","    "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ov7EPe_eqmbP"},"outputs":[],"source":["import random\n","\n","def predict(model, dataset, tkn, h=None):\n","         \n","  # tensor inputs\n","  x = np.array([[dataset.tokenizer.word_index[tkn]]])\n","  inputs = torch.from_numpy(x)\n","  \n","  # push to GPU\n","  inputs = inputs.cuda()\n","\n","  # detach hidden state from history\n","  h = tuple([each.data for each in h])\n","\n","  # get the output of the model\n","  out, h = model(inputs, h)\n","\n","  # get the token probabilities\n","  p = F.softmax(out, dim=1).data\n","\n","  p = p.cpu()\n","\n","  p = p.numpy()\n","  p = p.reshape(p.shape[1],)\n","\n","  # get indices of top 3 values\n","  top_n_idx = p.argsort()[-3:][::-1]\n","\n","  # randomly select one of the three indices\n","  sampled_token_index = top_n_idx[random.sample([0,1,2],1)[0]]\n","\n","  # return the encoded value of the predicted char and the hidden state\n","  return dataset.tokenizer.index_word[sampled_token_index], h\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"64k_4kiFFwec"},"outputs":[],"source":["# function to generate text\n","def sample(model, dataset, size, device, initial):\n","    \n","    model.eval()\n","\n","    # batch size is 1\n","    h = model.init_hidden(batchSize=1)\n","\n","    toks = initial\n","    title = []\n","\n","    # predict next token\n","    for t in initial:\n","      token, h = predict(model, dataset, t, h)\n","    \n","    toks.append(token)\n","\n","    title.append(token)\n","\n","    # predict subsequent tokens\n","    for i in range(size-1):\n","        token, h = predict(model, dataset, toks[-1], h)\n","        toks.append(token)\n","        title.append(token)\n","\n","    return ' '.join(title)"]},{"cell_type":"markdown","metadata":{"id":"fdDLkBBLbNiz"},"source":["## Execution"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dHdjyULcN1d1"},"outputs":[],"source":["hyperParams = HyperParams(epochs=30, batchSize=32)\n","print(hyperParams)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xWe4t2xp_Ieg"},"outputs":[],"source":["titleSet = TitleDataset(hyperParams, baseFrame)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"S9gj2kBIJO-J"},"outputs":[],"source":["pd.DataFrame.from_dict(pd.Series(titleSet.tokenizer.word_index))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dJjIRoZ5YxWm"},"outputs":[],"source":["trainNum = int(hyperParams.ratio[0] * len(titleSet))\n","valNum = int(hyperParams.ratio[1] * len(titleSet))\n","testNum = len(titleSet) - trainNum - valNum\n","splitSet = random_split(titleSet, [trainNum, valNum, testNum], generator=torch.Generator().manual_seed(0))\n","splitSet = {'train': splitSet[0], 'val': splitSet[1], 'test': splitSet[2]}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7cAHwy4Y13_r"},"outputs":[],"source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(torch.cuda.get_device_name(0))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oOYpjAPHE3HY"},"outputs":[],"source":["model = EmbedLSTM(hyperParams, titleSet, device)\n","model.to(device)\n","print(model)\n","# summary(model, (16,53))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CL4vqYn0aXXk"},"outputs":[],"source":["train(splitSet, model, hyperParams, device)"]},{"cell_type":"markdown","metadata":{"id":"55vwp4AQDsru"},"source":["## Prediction"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"foZTcBSWXDGp"},"outputs":[],"source":["# sample(model, titleSet, 6, device, initial=['dry', 'penne', 'pasta', 'broccoli', 'sun', 'dried', 'tomatoes', 'packed', 'in', 'oil', 'garlic', 'cloves', 'cheddar', 'cheese', 'salt', 'black', 'pepper'])\n","seq = splitSet['test'][np.random.randint(0, len(splitSet['test']))][0].tolist()\n","\n","def remove_values_from_list(the_list, val):\n","   return [titleSet.tokenizer.index_word[value] for value in the_list if value != val]\n","\n","seq = remove_values_from_list(seq, titleSet.tokenizer.word_index['OOV'])\n","print(seq)\n","\n","sample(model, titleSet, 6, device, initial=seq)\n","\n","## DOWNSIDE of this splitting: other information is hard to obtain (lost during random split), therefore --> split up frame before passing it to titleSet class"]},{"cell_type":"markdown","metadata":{"id":"ViDsBEbBtFcW"},"source":["https://towardsdatascience.com/lstm-for-time-series-prediction-de8aeb26f2ca for input sequence"]},{"cell_type":"markdown","metadata":{"id":"IaojBmezDulc"},"source":["## Save Model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pro0xtZxFMuq"},"outputs":[],"source":["# Save weights\n","torch.save(model.state_dict(), '/content/drive/MyDrive/weights/titleGenerator_model.pt')"]},{"cell_type":"markdown","metadata":{"id":"XG3G4d4KYlxj"},"source":["# Tensorboard visualization\n","\n","* https://pytorch.org/docs/stable/tensorboard.html\n","* https://github.com/christianversloot/machine-learning-articles/blob/main/how-to-use-tensorboard-with-pytorch.md"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NgcJKS4lfsY0"},"outputs":[],"source":["%load_ext tensorboard\n","%tensorboard --logdir=/content/drive/MyDrive/runs/titleTrainer"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DhDetWcvllGW"},"outputs":[],"source":[""]}],"metadata":{"accelerator":"GPU","colab":{"background_execution":"on","collapsed_sections":[],"machine_shape":"hm","name":"titleGen.ipynb","provenance":[{"file_id":"1pumjl0TZogLjixZkU705rl0G0Tw4XJqw","timestamp":1647095713433}],"authorship_tag":"ABX9TyM9GglRnxo1XgIR0nvwNFzA"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}